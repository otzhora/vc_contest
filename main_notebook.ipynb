{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "import json \n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import parallel_backend\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('robot_data/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['year'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>robot_gear_compression_diff_1</th>\n",
       "      <th>weapon_robot_armour_index_2</th>\n",
       "      <th>robot_gear_compression_diff_3</th>\n",
       "      <th>robot_gear_compression_diff_4</th>\n",
       "      <th>weapon_robot_punch_right_1</th>\n",
       "      <th>robot_gear_compression_diff_6</th>\n",
       "      <th>robot_gear_compression_diff_7</th>\n",
       "      <th>robot_gear_compression_diff_8</th>\n",
       "      <th>robot_gear_compression_diff_9</th>\n",
       "      <th>robot_gear_compression_diff_10</th>\n",
       "      <th>robot_gear_circulation_1</th>\n",
       "      <th>robot_gear_circulation_2</th>\n",
       "      <th>weapon_robot_punch_left_3</th>\n",
       "      <th>weapon_robot_armour_index_5</th>\n",
       "      <th>weapon_robot_armour_index_3</th>\n",
       "      <th>robot_gear_circulation_6</th>\n",
       "      <th>weapon_robot_punch_right_4</th>\n",
       "      <th>robot_gear_circulation_8</th>\n",
       "      <th>weapon_robot_punch_right_2</th>\n",
       "      <th>weapon_robot_gun_power_4</th>\n",
       "      <th>weapon_robot_gun_power_5</th>\n",
       "      <th>robot_gear_circulation_12</th>\n",
       "      <th>robot_gear_circulation_13</th>\n",
       "      <th>robot_gear_circulation_14</th>\n",
       "      <th>robot_gear_circulation_15</th>\n",
       "      <th>weapon_robot_gun_power_3</th>\n",
       "      <th>weapon_robot_punch_left_5</th>\n",
       "      <th>robot_gear_circulation_18</th>\n",
       "      <th>robot_gear_circulation_19</th>\n",
       "      <th>robot_gear_circulation_20</th>\n",
       "      <th>robot_gear_circulation_21</th>\n",
       "      <th>weapon_robot_punch_right_5</th>\n",
       "      <th>robot_gear_coef_1</th>\n",
       "      <th>weapon_robot_gun_power_2</th>\n",
       "      <th>robot_gear_compression_1</th>\n",
       "      <th>robot_gear_compression_2</th>\n",
       "      <th>robot_gear_compression_3</th>\n",
       "      <th>weapon_robot_armour_index_4</th>\n",
       "      <th>weapon_robot_eye_laser_emission_1</th>\n",
       "      <th>robot_gear_compression_6</th>\n",
       "      <th>robot_gear_temperature_diff_1</th>\n",
       "      <th>robot_gear_temperature_diff_2</th>\n",
       "      <th>robot_gear_temperature_diff_3</th>\n",
       "      <th>weapon_robot_eye_laser_sensor_1</th>\n",
       "      <th>robot_gear_temperature_diff_5</th>\n",
       "      <th>robot_gear_temperature_diff_6</th>\n",
       "      <th>robot_gear_temperature_1</th>\n",
       "      <th>robot_gear_temperature_2</th>\n",
       "      <th>robot_gear_temperature_3</th>\n",
       "      <th>robot_gear_temperature_4</th>\n",
       "      <th>weapon_robot_eye_laser_emission_4</th>\n",
       "      <th>weapon_robot_punch_right_3</th>\n",
       "      <th>weapon_robot_gun_power_1</th>\n",
       "      <th>robot_gear_temperature_8</th>\n",
       "      <th>robot_gear_temperature_9</th>\n",
       "      <th>robot_gear_temperature_10</th>\n",
       "      <th>robot_gear_temperature_11</th>\n",
       "      <th>robot_gear_temperature_12</th>\n",
       "      <th>robot_gear_temperature_13</th>\n",
       "      <th>robot_gear_temperature_14</th>\n",
       "      <th>robotic_circuits_speed_1</th>\n",
       "      <th>robotic_circuits_speed_2</th>\n",
       "      <th>robotic_circuits_speed_3</th>\n",
       "      <th>robotic_circuits_speed_4</th>\n",
       "      <th>robotic_circuits_speed_5</th>\n",
       "      <th>robotic_circuits_speed_6</th>\n",
       "      <th>robotic_circuits_speed_12</th>\n",
       "      <th>robot_engine_speed_13</th>\n",
       "      <th>robot_engine_speed_14</th>\n",
       "      <th>robot_engine_speed_15</th>\n",
       "      <th>robot_engine_speed_16</th>\n",
       "      <th>robot_engine_circulation_2</th>\n",
       "      <th>robot_engine_circulation_3</th>\n",
       "      <th>robot_engine_circulation_4</th>\n",
       "      <th>robot_engine_circulation_6</th>\n",
       "      <th>robot_engine_circulation_7</th>\n",
       "      <th>robot_engine_circulation_8</th>\n",
       "      <th>robot_engine_ground_1</th>\n",
       "      <th>robot_engine_compression_1</th>\n",
       "      <th>robot_engine_compression_2</th>\n",
       "      <th>robot_engine_compression_3</th>\n",
       "      <th>weapon_robot_eye_laser_emission_2</th>\n",
       "      <th>robot_engine_temperature_2</th>\n",
       "      <th>robot_engine_temperature_3</th>\n",
       "      <th>robot_engine_temperature_4</th>\n",
       "      <th>robot_engine_temperature_5</th>\n",
       "      <th>robot_engine_temperature_6</th>\n",
       "      <th>weapon_robot_eye_laser_sensor_2</th>\n",
       "      <th>weapon_robot_punch_left_1</th>\n",
       "      <th>robot_engine_temperature_9</th>\n",
       "      <th>robot_engine_temperature_10</th>\n",
       "      <th>robot_engine_temperature_11</th>\n",
       "      <th>robot_engine_temperature_12</th>\n",
       "      <th>robot_engine_temperature_13</th>\n",
       "      <th>robot_engine_temperature_14</th>\n",
       "      <th>robot_engine_temperature_15</th>\n",
       "      <th>robot_engine_temperature_16</th>\n",
       "      <th>robot_engine_temperature_17</th>\n",
       "      <th>weapon_robot_eye_laser_range_2</th>\n",
       "      <th>weapon_robot_eye_laser_sensor_3</th>\n",
       "      <th>robot_engine_temperature_20</th>\n",
       "      <th>robot_engine_temperature_21</th>\n",
       "      <th>weapon_robot_eye_laser_emission_3</th>\n",
       "      <th>robot_engine_temperature_23</th>\n",
       "      <th>robot_engine_temperature_24</th>\n",
       "      <th>robot_engine_temperature_25</th>\n",
       "      <th>robot_engine_temperature_26</th>\n",
       "      <th>robot_engine_temperature_27</th>\n",
       "      <th>robot_engine_temperature_28</th>\n",
       "      <th>robot_probe_compression_diff_1</th>\n",
       "      <th>robot_probe_compression_diff_2</th>\n",
       "      <th>robot_probe_compression_diff_3</th>\n",
       "      <th>robot_probe_compression_diff_4</th>\n",
       "      <th>robot_probe_compression_diff_5</th>\n",
       "      <th>robot_probe_compression_diff_6</th>\n",
       "      <th>robot_probe_compression_diff_7</th>\n",
       "      <th>robot_probe_compression_diff_8</th>\n",
       "      <th>robot_probe_compression_diff_9</th>\n",
       "      <th>robot_probe_compression_diff_10</th>\n",
       "      <th>robot_probe_circulation_1</th>\n",
       "      <th>robot_probe_circulation_2</th>\n",
       "      <th>robot_probe_circulation_3</th>\n",
       "      <th>robot_probe_circulation_4</th>\n",
       "      <th>robot_probe_circulation_5</th>\n",
       "      <th>robot_probe_circulation_6</th>\n",
       "      <th>robot_probe_circulation_7</th>\n",
       "      <th>weapon_robot_armour_index_1</th>\n",
       "      <th>robot_probe_circulation_9</th>\n",
       "      <th>robot_probe_circulation_10</th>\n",
       "      <th>robot_probe_circulation_11</th>\n",
       "      <th>robot_probe_circulation_12</th>\n",
       "      <th>robot_probe_temperature_1</th>\n",
       "      <th>robot_probe_temperature_2</th>\n",
       "      <th>robot_probe_temperature_3</th>\n",
       "      <th>weapon_robot_eye_laser_sensor_4</th>\n",
       "      <th>robot_probe_temperature_5</th>\n",
       "      <th>robot_probe_temperature_6</th>\n",
       "      <th>robot_probe_temperature_7</th>\n",
       "      <th>robot_probe_temperature_8</th>\n",
       "      <th>robot_probe_temperature_9</th>\n",
       "      <th>weapon_robot_eye_laser_range_1</th>\n",
       "      <th>weapon_robot_punch_left_4</th>\n",
       "      <th>weapon_robot_punch_left_2</th>\n",
       "      <th>gamma_ray</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.904224</td>\n",
       "      <td>15.166449</td>\n",
       "      <td>16.411567</td>\n",
       "      <td>26.384177</td>\n",
       "      <td>18.747389</td>\n",
       "      <td>12.887911</td>\n",
       "      <td>18.491861</td>\n",
       "      <td>15.792127</td>\n",
       "      <td>7.670282</td>\n",
       "      <td>28.774358</td>\n",
       "      <td>9.909089</td>\n",
       "      <td>40.766893</td>\n",
       "      <td>22.773625</td>\n",
       "      <td>47.470404</td>\n",
       "      <td>46.674830</td>\n",
       "      <td>40.145648</td>\n",
       "      <td>10.898802</td>\n",
       "      <td>32.424613</td>\n",
       "      <td>34.632175</td>\n",
       "      <td>48.607230</td>\n",
       "      <td>1.189888</td>\n",
       "      <td>-6.183945</td>\n",
       "      <td>9.927305</td>\n",
       "      <td>1.512784</td>\n",
       "      <td>0.961644</td>\n",
       "      <td>0.905874</td>\n",
       "      <td>14.235658</td>\n",
       "      <td>40.609019</td>\n",
       "      <td>-14.548193</td>\n",
       "      <td>-13.181533</td>\n",
       "      <td>4.336607</td>\n",
       "      <td>13.949694</td>\n",
       "      <td>3.227183</td>\n",
       "      <td>25.273701</td>\n",
       "      <td>15.714763</td>\n",
       "      <td>12.167470</td>\n",
       "      <td>25.026348</td>\n",
       "      <td>41.157891</td>\n",
       "      <td>48.374696</td>\n",
       "      <td>25.147236</td>\n",
       "      <td>-6.224324</td>\n",
       "      <td>-6.924619</td>\n",
       "      <td>-3.000026</td>\n",
       "      <td>-4.751072</td>\n",
       "      <td>-30.497994</td>\n",
       "      <td>-16.426079</td>\n",
       "      <td>0.956318</td>\n",
       "      <td>1.065475</td>\n",
       "      <td>19.065808</td>\n",
       "      <td>18.473386</td>\n",
       "      <td>9.049724</td>\n",
       "      <td>2.178295</td>\n",
       "      <td>3.434999</td>\n",
       "      <td>6.635133</td>\n",
       "      <td>3.330549</td>\n",
       "      <td>1.377578</td>\n",
       "      <td>2.630183</td>\n",
       "      <td>6.660853</td>\n",
       "      <td>-44.265133</td>\n",
       "      <td>6.905124</td>\n",
       "      <td>-10.230125</td>\n",
       "      <td>14.086136</td>\n",
       "      <td>33.337482</td>\n",
       "      <td>22.082856</td>\n",
       "      <td>-2.161208</td>\n",
       "      <td>46.874595</td>\n",
       "      <td>-2.885696</td>\n",
       "      <td>3.677319</td>\n",
       "      <td>26.307353</td>\n",
       "      <td>-0.626873</td>\n",
       "      <td>-1.523823</td>\n",
       "      <td>-3.798114</td>\n",
       "      <td>5.651547</td>\n",
       "      <td>-19.019474</td>\n",
       "      <td>-0.784865</td>\n",
       "      <td>3.748141</td>\n",
       "      <td>9.118077</td>\n",
       "      <td>17.215802</td>\n",
       "      <td>-9.809502</td>\n",
       "      <td>-12.386069</td>\n",
       "      <td>3.611320</td>\n",
       "      <td>5.537812</td>\n",
       "      <td>5.397028</td>\n",
       "      <td>-12.803319</td>\n",
       "      <td>19.898864</td>\n",
       "      <td>-13.227218</td>\n",
       "      <td>9.852316</td>\n",
       "      <td>2.333973</td>\n",
       "      <td>-9.120435</td>\n",
       "      <td>3.092688</td>\n",
       "      <td>13.504738</td>\n",
       "      <td>17.023118</td>\n",
       "      <td>6.122366</td>\n",
       "      <td>13.702791</td>\n",
       "      <td>13.991228</td>\n",
       "      <td>15.650985</td>\n",
       "      <td>13.576761</td>\n",
       "      <td>20.326959</td>\n",
       "      <td>9.138219</td>\n",
       "      <td>2.245576</td>\n",
       "      <td>-0.587865</td>\n",
       "      <td>-2.595478</td>\n",
       "      <td>-3.236517</td>\n",
       "      <td>-1.838056</td>\n",
       "      <td>-4.073338</td>\n",
       "      <td>-4.074009</td>\n",
       "      <td>6.437224</td>\n",
       "      <td>10.391109</td>\n",
       "      <td>-4.669151</td>\n",
       "      <td>-23.491741</td>\n",
       "      <td>32.100224</td>\n",
       "      <td>16.940023</td>\n",
       "      <td>34.886572</td>\n",
       "      <td>9.543415</td>\n",
       "      <td>-30.723220</td>\n",
       "      <td>-13.600814</td>\n",
       "      <td>14.850165</td>\n",
       "      <td>-3.850033</td>\n",
       "      <td>-22.649161</td>\n",
       "      <td>-13.348838</td>\n",
       "      <td>1.579993</td>\n",
       "      <td>27.982304</td>\n",
       "      <td>-21.063083</td>\n",
       "      <td>-27.096981</td>\n",
       "      <td>2.654190</td>\n",
       "      <td>1.357739</td>\n",
       "      <td>1.155576</td>\n",
       "      <td>0.630935</td>\n",
       "      <td>1.623904</td>\n",
       "      <td>1.497468</td>\n",
       "      <td>3.269319</td>\n",
       "      <td>-12.954699</td>\n",
       "      <td>1.627971</td>\n",
       "      <td>-11.545831</td>\n",
       "      <td>0.528241</td>\n",
       "      <td>2.108404</td>\n",
       "      <td>1.194372</td>\n",
       "      <td>0.665295</td>\n",
       "      <td>3.078837</td>\n",
       "      <td>1.911756</td>\n",
       "      <td>1.685329</td>\n",
       "      <td>-4.555385</td>\n",
       "      <td>10.610730</td>\n",
       "      <td>moderate</td>\n",
       "      <td>20.396055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.829009</td>\n",
       "      <td>14.817183</td>\n",
       "      <td>16.353650</td>\n",
       "      <td>24.191974</td>\n",
       "      <td>22.313258</td>\n",
       "      <td>13.108823</td>\n",
       "      <td>18.451640</td>\n",
       "      <td>16.234554</td>\n",
       "      <td>9.722622</td>\n",
       "      <td>24.524993</td>\n",
       "      <td>9.314892</td>\n",
       "      <td>20.851960</td>\n",
       "      <td>40.646285</td>\n",
       "      <td>47.190100</td>\n",
       "      <td>45.877504</td>\n",
       "      <td>39.890871</td>\n",
       "      <td>12.112552</td>\n",
       "      <td>36.327078</td>\n",
       "      <td>39.331091</td>\n",
       "      <td>49.922699</td>\n",
       "      <td>-3.476083</td>\n",
       "      <td>-6.186788</td>\n",
       "      <td>9.929770</td>\n",
       "      <td>1.461015</td>\n",
       "      <td>0.918912</td>\n",
       "      <td>0.835761</td>\n",
       "      <td>13.932550</td>\n",
       "      <td>42.500281</td>\n",
       "      <td>-15.045941</td>\n",
       "      <td>-12.694736</td>\n",
       "      <td>4.309321</td>\n",
       "      <td>14.778146</td>\n",
       "      <td>4.063043</td>\n",
       "      <td>51.778912</td>\n",
       "      <td>15.755389</td>\n",
       "      <td>11.629879</td>\n",
       "      <td>25.279591</td>\n",
       "      <td>40.714217</td>\n",
       "      <td>52.230417</td>\n",
       "      <td>43.700264</td>\n",
       "      <td>-8.569177</td>\n",
       "      <td>-11.830147</td>\n",
       "      <td>-9.826176</td>\n",
       "      <td>-6.200095</td>\n",
       "      <td>-41.513246</td>\n",
       "      <td>-19.917287</td>\n",
       "      <td>-1.202393</td>\n",
       "      <td>-1.018839</td>\n",
       "      <td>13.103415</td>\n",
       "      <td>17.547540</td>\n",
       "      <td>8.198577</td>\n",
       "      <td>0.805081</td>\n",
       "      <td>3.768630</td>\n",
       "      <td>3.221765</td>\n",
       "      <td>3.396309</td>\n",
       "      <td>0.142532</td>\n",
       "      <td>1.193677</td>\n",
       "      <td>2.514940</td>\n",
       "      <td>-46.970675</td>\n",
       "      <td>-23.793164</td>\n",
       "      <td>-40.601749</td>\n",
       "      <td>9.165616</td>\n",
       "      <td>32.006672</td>\n",
       "      <td>13.626746</td>\n",
       "      <td>-2.161208</td>\n",
       "      <td>42.012949</td>\n",
       "      <td>-2.885696</td>\n",
       "      <td>-1.572028</td>\n",
       "      <td>25.027540</td>\n",
       "      <td>-0.708183</td>\n",
       "      <td>-1.265149</td>\n",
       "      <td>-3.798114</td>\n",
       "      <td>5.651547</td>\n",
       "      <td>-12.082857</td>\n",
       "      <td>-0.784865</td>\n",
       "      <td>3.540441</td>\n",
       "      <td>9.123438</td>\n",
       "      <td>38.791165</td>\n",
       "      <td>-8.268191</td>\n",
       "      <td>-27.073257</td>\n",
       "      <td>2.047121</td>\n",
       "      <td>3.167220</td>\n",
       "      <td>1.968319</td>\n",
       "      <td>-12.883382</td>\n",
       "      <td>-1.786862</td>\n",
       "      <td>-17.070863</td>\n",
       "      <td>7.609704</td>\n",
       "      <td>2.176281</td>\n",
       "      <td>5.049620</td>\n",
       "      <td>10.705052</td>\n",
       "      <td>24.745531</td>\n",
       "      <td>23.818490</td>\n",
       "      <td>7.307127</td>\n",
       "      <td>14.968410</td>\n",
       "      <td>15.358290</td>\n",
       "      <td>16.203016</td>\n",
       "      <td>15.032753</td>\n",
       "      <td>20.694469</td>\n",
       "      <td>9.524987</td>\n",
       "      <td>2.245576</td>\n",
       "      <td>-1.005475</td>\n",
       "      <td>-1.719989</td>\n",
       "      <td>-2.857909</td>\n",
       "      <td>2.387616</td>\n",
       "      <td>-2.224259</td>\n",
       "      <td>-4.074009</td>\n",
       "      <td>6.226098</td>\n",
       "      <td>10.391109</td>\n",
       "      <td>-8.844031</td>\n",
       "      <td>-23.821588</td>\n",
       "      <td>43.997752</td>\n",
       "      <td>32.109511</td>\n",
       "      <td>48.954852</td>\n",
       "      <td>7.319419</td>\n",
       "      <td>-3.106639</td>\n",
       "      <td>10.872032</td>\n",
       "      <td>2.869523</td>\n",
       "      <td>6.673960</td>\n",
       "      <td>4.463331</td>\n",
       "      <td>11.561926</td>\n",
       "      <td>1.445675</td>\n",
       "      <td>37.991359</td>\n",
       "      <td>-26.046032</td>\n",
       "      <td>-34.821047</td>\n",
       "      <td>2.440002</td>\n",
       "      <td>1.001539</td>\n",
       "      <td>1.366884</td>\n",
       "      <td>0.664777</td>\n",
       "      <td>2.060766</td>\n",
       "      <td>1.639949</td>\n",
       "      <td>3.200206</td>\n",
       "      <td>-13.394192</td>\n",
       "      <td>2.107142</td>\n",
       "      <td>2.111107</td>\n",
       "      <td>0.956559</td>\n",
       "      <td>4.116632</td>\n",
       "      <td>2.186736</td>\n",
       "      <td>0.574192</td>\n",
       "      <td>3.209856</td>\n",
       "      <td>1.546033</td>\n",
       "      <td>1.549280</td>\n",
       "      <td>-1.644398</td>\n",
       "      <td>-6.876256</td>\n",
       "      <td>moderate</td>\n",
       "      <td>18.413807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.785779</td>\n",
       "      <td>4.952384</td>\n",
       "      <td>5.133674</td>\n",
       "      <td>13.453927</td>\n",
       "      <td>-2.626315</td>\n",
       "      <td>4.615422</td>\n",
       "      <td>3.985793</td>\n",
       "      <td>1.910353</td>\n",
       "      <td>5.015227</td>\n",
       "      <td>5.903975</td>\n",
       "      <td>2.608068</td>\n",
       "      <td>-5.351592</td>\n",
       "      <td>11.667464</td>\n",
       "      <td>11.030628</td>\n",
       "      <td>10.735017</td>\n",
       "      <td>8.813446</td>\n",
       "      <td>3.985458</td>\n",
       "      <td>11.599129</td>\n",
       "      <td>12.329768</td>\n",
       "      <td>18.530218</td>\n",
       "      <td>-12.447955</td>\n",
       "      <td>-6.193296</td>\n",
       "      <td>10.136860</td>\n",
       "      <td>2.612884</td>\n",
       "      <td>0.974218</td>\n",
       "      <td>0.906767</td>\n",
       "      <td>3.026846</td>\n",
       "      <td>-20.597346</td>\n",
       "      <td>-15.103679</td>\n",
       "      <td>-13.632254</td>\n",
       "      <td>4.540783</td>\n",
       "      <td>13.691721</td>\n",
       "      <td>-1.312072</td>\n",
       "      <td>-11.678518</td>\n",
       "      <td>2.748269</td>\n",
       "      <td>2.473859</td>\n",
       "      <td>6.570108</td>\n",
       "      <td>11.126349</td>\n",
       "      <td>8.811222</td>\n",
       "      <td>-11.197065</td>\n",
       "      <td>-5.029335</td>\n",
       "      <td>-3.192665</td>\n",
       "      <td>-3.808756</td>\n",
       "      <td>-3.201062</td>\n",
       "      <td>-37.099966</td>\n",
       "      <td>-11.377644</td>\n",
       "      <td>5.747418</td>\n",
       "      <td>8.982241</td>\n",
       "      <td>10.314317</td>\n",
       "      <td>14.657907</td>\n",
       "      <td>5.416813</td>\n",
       "      <td>2.931476</td>\n",
       "      <td>2.838831</td>\n",
       "      <td>2.859771</td>\n",
       "      <td>3.371686</td>\n",
       "      <td>0.270928</td>\n",
       "      <td>2.286544</td>\n",
       "      <td>3.984483</td>\n",
       "      <td>54.851068</td>\n",
       "      <td>36.395228</td>\n",
       "      <td>11.660536</td>\n",
       "      <td>-14.076600</td>\n",
       "      <td>-13.262270</td>\n",
       "      <td>-6.245913</td>\n",
       "      <td>-2.161208</td>\n",
       "      <td>26.133265</td>\n",
       "      <td>-2.885696</td>\n",
       "      <td>8.591901</td>\n",
       "      <td>-56.866149</td>\n",
       "      <td>-0.576700</td>\n",
       "      <td>-2.464071</td>\n",
       "      <td>-3.798114</td>\n",
       "      <td>5.651547</td>\n",
       "      <td>-5.999977</td>\n",
       "      <td>-0.784865</td>\n",
       "      <td>3.893099</td>\n",
       "      <td>7.561725</td>\n",
       "      <td>-32.677466</td>\n",
       "      <td>5.215334</td>\n",
       "      <td>43.124856</td>\n",
       "      <td>3.876680</td>\n",
       "      <td>3.137828</td>\n",
       "      <td>6.943892</td>\n",
       "      <td>-7.015461</td>\n",
       "      <td>-32.619928</td>\n",
       "      <td>-28.669681</td>\n",
       "      <td>4.155563</td>\n",
       "      <td>2.106221</td>\n",
       "      <td>-11.744209</td>\n",
       "      <td>1.733197</td>\n",
       "      <td>9.542481</td>\n",
       "      <td>14.754550</td>\n",
       "      <td>5.877511</td>\n",
       "      <td>14.281714</td>\n",
       "      <td>14.262525</td>\n",
       "      <td>16.538149</td>\n",
       "      <td>15.140687</td>\n",
       "      <td>21.570683</td>\n",
       "      <td>9.784028</td>\n",
       "      <td>3.445695</td>\n",
       "      <td>3.537749</td>\n",
       "      <td>0.603608</td>\n",
       "      <td>0.596065</td>\n",
       "      <td>-3.247204</td>\n",
       "      <td>-0.374597</td>\n",
       "      <td>-0.515163</td>\n",
       "      <td>6.242417</td>\n",
       "      <td>11.315056</td>\n",
       "      <td>12.018977</td>\n",
       "      <td>7.086985</td>\n",
       "      <td>10.114534</td>\n",
       "      <td>-22.089317</td>\n",
       "      <td>-41.447510</td>\n",
       "      <td>-3.637141</td>\n",
       "      <td>-6.422254</td>\n",
       "      <td>-30.942248</td>\n",
       "      <td>-0.275010</td>\n",
       "      <td>0.924829</td>\n",
       "      <td>-79.105138</td>\n",
       "      <td>10.552648</td>\n",
       "      <td>1.051720</td>\n",
       "      <td>0.706075</td>\n",
       "      <td>15.283965</td>\n",
       "      <td>34.729677</td>\n",
       "      <td>1.949048</td>\n",
       "      <td>1.293275</td>\n",
       "      <td>0.690178</td>\n",
       "      <td>0.825595</td>\n",
       "      <td>1.672130</td>\n",
       "      <td>1.823550</td>\n",
       "      <td>3.248624</td>\n",
       "      <td>-1.142748</td>\n",
       "      <td>-1.367254</td>\n",
       "      <td>9.581747</td>\n",
       "      <td>0.956559</td>\n",
       "      <td>8.595773</td>\n",
       "      <td>5.379803</td>\n",
       "      <td>-0.817837</td>\n",
       "      <td>3.117562</td>\n",
       "      <td>2.022720</td>\n",
       "      <td>1.760883</td>\n",
       "      <td>-0.647793</td>\n",
       "      <td>-50.334014</td>\n",
       "      <td>moderate</td>\n",
       "      <td>3.181423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.280151</td>\n",
       "      <td>14.404758</td>\n",
       "      <td>16.427092</td>\n",
       "      <td>28.512888</td>\n",
       "      <td>16.987524</td>\n",
       "      <td>12.068514</td>\n",
       "      <td>17.722866</td>\n",
       "      <td>16.117127</td>\n",
       "      <td>15.303988</td>\n",
       "      <td>22.071968</td>\n",
       "      <td>14.270098</td>\n",
       "      <td>1.093200</td>\n",
       "      <td>58.364581</td>\n",
       "      <td>46.027743</td>\n",
       "      <td>45.002611</td>\n",
       "      <td>38.884433</td>\n",
       "      <td>9.715656</td>\n",
       "      <td>29.313836</td>\n",
       "      <td>31.036017</td>\n",
       "      <td>41.025708</td>\n",
       "      <td>-3.630609</td>\n",
       "      <td>-6.186887</td>\n",
       "      <td>9.919465</td>\n",
       "      <td>1.908753</td>\n",
       "      <td>1.003060</td>\n",
       "      <td>0.959420</td>\n",
       "      <td>13.780261</td>\n",
       "      <td>40.088523</td>\n",
       "      <td>-14.929992</td>\n",
       "      <td>-13.469522</td>\n",
       "      <td>4.472582</td>\n",
       "      <td>13.632497</td>\n",
       "      <td>-0.695668</td>\n",
       "      <td>2.707336</td>\n",
       "      <td>16.324431</td>\n",
       "      <td>12.635649</td>\n",
       "      <td>26.640917</td>\n",
       "      <td>46.356996</td>\n",
       "      <td>61.995778</td>\n",
       "      <td>76.256865</td>\n",
       "      <td>-5.789522</td>\n",
       "      <td>-3.514938</td>\n",
       "      <td>-2.593691</td>\n",
       "      <td>-0.230709</td>\n",
       "      <td>-27.300900</td>\n",
       "      <td>-16.881916</td>\n",
       "      <td>-0.868214</td>\n",
       "      <td>1.191245</td>\n",
       "      <td>11.983860</td>\n",
       "      <td>16.599363</td>\n",
       "      <td>7.955884</td>\n",
       "      <td>2.372484</td>\n",
       "      <td>3.151000</td>\n",
       "      <td>5.539438</td>\n",
       "      <td>2.729064</td>\n",
       "      <td>0.838315</td>\n",
       "      <td>1.937454</td>\n",
       "      <td>5.171496</td>\n",
       "      <td>-14.866713</td>\n",
       "      <td>-18.211608</td>\n",
       "      <td>11.961520</td>\n",
       "      <td>7.454739</td>\n",
       "      <td>19.540050</td>\n",
       "      <td>25.220135</td>\n",
       "      <td>-2.161208</td>\n",
       "      <td>49.101212</td>\n",
       "      <td>-2.885696</td>\n",
       "      <td>-2.073189</td>\n",
       "      <td>-1.661497</td>\n",
       "      <td>-0.620986</td>\n",
       "      <td>-1.635986</td>\n",
       "      <td>-3.798114</td>\n",
       "      <td>2.735514</td>\n",
       "      <td>-14.964221</td>\n",
       "      <td>-0.784865</td>\n",
       "      <td>3.862554</td>\n",
       "      <td>10.814516</td>\n",
       "      <td>-16.856149</td>\n",
       "      <td>-2.101216</td>\n",
       "      <td>23.395379</td>\n",
       "      <td>5.656382</td>\n",
       "      <td>2.601427</td>\n",
       "      <td>9.565148</td>\n",
       "      <td>-10.946502</td>\n",
       "      <td>-10.177850</td>\n",
       "      <td>-19.854531</td>\n",
       "      <td>7.340578</td>\n",
       "      <td>2.535338</td>\n",
       "      <td>2.442275</td>\n",
       "      <td>9.860105</td>\n",
       "      <td>27.792571</td>\n",
       "      <td>28.820591</td>\n",
       "      <td>9.336510</td>\n",
       "      <td>18.843183</td>\n",
       "      <td>20.252962</td>\n",
       "      <td>21.243331</td>\n",
       "      <td>19.959753</td>\n",
       "      <td>27.259568</td>\n",
       "      <td>11.920439</td>\n",
       "      <td>3.745677</td>\n",
       "      <td>0.438746</td>\n",
       "      <td>-1.909155</td>\n",
       "      <td>-2.870308</td>\n",
       "      <td>2.387616</td>\n",
       "      <td>-4.073338</td>\n",
       "      <td>-9.411717</td>\n",
       "      <td>8.704193</td>\n",
       "      <td>15.011473</td>\n",
       "      <td>-13.013215</td>\n",
       "      <td>-13.779791</td>\n",
       "      <td>6.782416</td>\n",
       "      <td>-11.205298</td>\n",
       "      <td>-18.188447</td>\n",
       "      <td>16.356786</td>\n",
       "      <td>-6.930751</td>\n",
       "      <td>-19.185316</td>\n",
       "      <td>12.357832</td>\n",
       "      <td>0.329683</td>\n",
       "      <td>-22.317412</td>\n",
       "      <td>47.207544</td>\n",
       "      <td>1.549449</td>\n",
       "      <td>13.951939</td>\n",
       "      <td>-21.543741</td>\n",
       "      <td>3.044392</td>\n",
       "      <td>2.772663</td>\n",
       "      <td>0.459583</td>\n",
       "      <td>0.308349</td>\n",
       "      <td>0.647667</td>\n",
       "      <td>1.612583</td>\n",
       "      <td>1.460540</td>\n",
       "      <td>3.241245</td>\n",
       "      <td>-16.083376</td>\n",
       "      <td>0.909051</td>\n",
       "      <td>10.494774</td>\n",
       "      <td>0.528241</td>\n",
       "      <td>3.799922</td>\n",
       "      <td>1.443960</td>\n",
       "      <td>0.674009</td>\n",
       "      <td>3.242115</td>\n",
       "      <td>0.904602</td>\n",
       "      <td>1.251439</td>\n",
       "      <td>-5.136062</td>\n",
       "      <td>-11.767418</td>\n",
       "      <td>moderate</td>\n",
       "      <td>17.878260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.558607</td>\n",
       "      <td>14.689644</td>\n",
       "      <td>15.482010</td>\n",
       "      <td>10.221080</td>\n",
       "      <td>20.796281</td>\n",
       "      <td>11.652582</td>\n",
       "      <td>20.178250</td>\n",
       "      <td>13.763336</td>\n",
       "      <td>-13.410173</td>\n",
       "      <td>20.173798</td>\n",
       "      <td>8.934834</td>\n",
       "      <td>8.886271</td>\n",
       "      <td>14.276316</td>\n",
       "      <td>47.738773</td>\n",
       "      <td>46.545421</td>\n",
       "      <td>40.253507</td>\n",
       "      <td>13.043598</td>\n",
       "      <td>39.223719</td>\n",
       "      <td>41.337693</td>\n",
       "      <td>52.177037</td>\n",
       "      <td>0.886695</td>\n",
       "      <td>-6.185725</td>\n",
       "      <td>9.913649</td>\n",
       "      <td>1.968300</td>\n",
       "      <td>0.968754</td>\n",
       "      <td>0.984168</td>\n",
       "      <td>14.038300</td>\n",
       "      <td>43.476943</td>\n",
       "      <td>-14.949634</td>\n",
       "      <td>-13.928786</td>\n",
       "      <td>4.035924</td>\n",
       "      <td>14.518674</td>\n",
       "      <td>3.896286</td>\n",
       "      <td>13.323275</td>\n",
       "      <td>16.527673</td>\n",
       "      <td>12.427569</td>\n",
       "      <td>27.495612</td>\n",
       "      <td>47.434892</td>\n",
       "      <td>63.206244</td>\n",
       "      <td>69.328477</td>\n",
       "      <td>-5.616545</td>\n",
       "      <td>-7.964505</td>\n",
       "      <td>-4.294930</td>\n",
       "      <td>-4.167853</td>\n",
       "      <td>3.313087</td>\n",
       "      <td>0.163748</td>\n",
       "      <td>8.131058</td>\n",
       "      <td>-0.506100</td>\n",
       "      <td>14.057789</td>\n",
       "      <td>17.410875</td>\n",
       "      <td>8.115805</td>\n",
       "      <td>3.045419</td>\n",
       "      <td>3.724217</td>\n",
       "      <td>5.970002</td>\n",
       "      <td>3.594084</td>\n",
       "      <td>1.770255</td>\n",
       "      <td>2.241501</td>\n",
       "      <td>8.886254</td>\n",
       "      <td>-4.130673</td>\n",
       "      <td>-17.609925</td>\n",
       "      <td>3.883447</td>\n",
       "      <td>0.590636</td>\n",
       "      <td>32.430530</td>\n",
       "      <td>21.673718</td>\n",
       "      <td>-2.161208</td>\n",
       "      <td>49.750244</td>\n",
       "      <td>-2.885696</td>\n",
       "      <td>-38.054783</td>\n",
       "      <td>491.708473</td>\n",
       "      <td>-0.499511</td>\n",
       "      <td>5.462909</td>\n",
       "      <td>-3.798114</td>\n",
       "      <td>-3.097597</td>\n",
       "      <td>181.898502</td>\n",
       "      <td>-0.784865</td>\n",
       "      <td>3.632604</td>\n",
       "      <td>10.778411</td>\n",
       "      <td>-44.876881</td>\n",
       "      <td>-10.799505</td>\n",
       "      <td>13.119058</td>\n",
       "      <td>3.421561</td>\n",
       "      <td>4.438767</td>\n",
       "      <td>0.932105</td>\n",
       "      <td>-11.719416</td>\n",
       "      <td>-13.688903</td>\n",
       "      <td>-18.034483</td>\n",
       "      <td>6.512499</td>\n",
       "      <td>2.599540</td>\n",
       "      <td>12.062265</td>\n",
       "      <td>12.511177</td>\n",
       "      <td>32.467110</td>\n",
       "      <td>33.230293</td>\n",
       "      <td>10.746183</td>\n",
       "      <td>21.187271</td>\n",
       "      <td>19.428493</td>\n",
       "      <td>16.426177</td>\n",
       "      <td>15.335713</td>\n",
       "      <td>19.179369</td>\n",
       "      <td>8.908758</td>\n",
       "      <td>0.145320</td>\n",
       "      <td>-3.949512</td>\n",
       "      <td>5.620597</td>\n",
       "      <td>-4.533750</td>\n",
       "      <td>15.063743</td>\n",
       "      <td>7.948006</td>\n",
       "      <td>-0.515163</td>\n",
       "      <td>8.832025</td>\n",
       "      <td>11.315056</td>\n",
       "      <td>-25.532159</td>\n",
       "      <td>-5.824100</td>\n",
       "      <td>18.771847</td>\n",
       "      <td>4.267911</td>\n",
       "      <td>11.083815</td>\n",
       "      <td>9.115312</td>\n",
       "      <td>7.577095</td>\n",
       "      <td>11.083795</td>\n",
       "      <td>14.268631</td>\n",
       "      <td>-1.015045</td>\n",
       "      <td>-10.325623</td>\n",
       "      <td>-26.416289</td>\n",
       "      <td>0.648198</td>\n",
       "      <td>-4.793615</td>\n",
       "      <td>-29.870106</td>\n",
       "      <td>8.941020</td>\n",
       "      <td>2.478081</td>\n",
       "      <td>1.751471</td>\n",
       "      <td>0.544837</td>\n",
       "      <td>0.695547</td>\n",
       "      <td>0.628867</td>\n",
       "      <td>2.028183</td>\n",
       "      <td>-1.214013</td>\n",
       "      <td>-15.500320</td>\n",
       "      <td>3.065810</td>\n",
       "      <td>-15.127357</td>\n",
       "      <td>0.813811</td>\n",
       "      <td>-43.104271</td>\n",
       "      <td>-38.797844</td>\n",
       "      <td>2.040819</td>\n",
       "      <td>3.027545</td>\n",
       "      <td>2.045397</td>\n",
       "      <td>1.550181</td>\n",
       "      <td>-2.279580</td>\n",
       "      <td>-23.324411</td>\n",
       "      <td>moderate</td>\n",
       "      <td>18.355902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   robot_gear_compression_diff_1  weapon_robot_armour_index_2  robot_gear_compression_diff_3    ...      weapon_robot_punch_left_2  gamma_ray     target\n",
       "0                      14.904224                    15.166449                      16.411567    ...                      10.610730   moderate  20.396055\n",
       "1                      14.829009                    14.817183                      16.353650    ...                      -6.876256   moderate  18.413807\n",
       "2                       5.785779                     4.952384                       5.133674    ...                     -50.334014   moderate   3.181423\n",
       "3                      14.280151                    14.404758                      16.427092    ...                     -11.767418   moderate  17.878260\n",
       "4                      14.558607                    14.689644                      15.482010    ...                     -23.324411   moderate  18.355902\n",
       "\n",
       "[5 rows x 145 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом датасете есть только 1 категориальная фича, которую мы можем кодировать или через onehot или прост сопоставить каждому значению чилсо"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode cat features with oneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def oneHotEncode(df,colNames):\n",
    "    for col in colNames:\n",
    "        if( df[col].dtype == np.dtype('object')):\n",
    "            dummies = pd.get_dummies(df[col],prefix=col)\n",
    "            df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "            #drop the encoded column\n",
    "            df.drop([col],axis = 1 , inplace=True)\n",
    "    return df\n",
    "\n",
    "# df_oh = oneHotEncode(df, ['gamma_ray'])\n",
    "# df_oh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode cat fetures with simple encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_encode(df, encode_map=None):\n",
    "    if not encode_map:\n",
    "        encode_map = {'low': 0.25, 'moderate': 0.5, 'high': 0.75, 'very high': 1}\n",
    "\n",
    "    df['gamma_ray'] = df['gamma_ray'].map(encode_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "# df_sim = simple_encode(df)\n",
    "# df_sim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем засплитить наш размеченный датасет на сет для тренировки и сет для теста модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сплит не в процентах, а в количестве семплов потому что бибу соси\n",
    "\n",
    "def split_train_test(df, split=600, enc_type='simple'):\n",
    "    if enc_type == 'simple':\n",
    "        df_ = simple_encode(df.copy())\n",
    "        X_train, y_train= df_.iloc[split:].drop(columns=['target'], inplace=False), df_.iloc[split:]['target']\n",
    "        X_test, y_test = df_.iloc[:split].drop(columns=['target'], inplace=False), df_.iloc[:split]['target']\n",
    "    elif enc_type == 'onehot':\n",
    "        df_ = oneHotEncode(df.copy(), ['gamma_ray'])\n",
    "        X_train, y_train = df_.iloc[split:].drop(columns=['target'], inplace=False), df_.iloc[split:]['target']\n",
    "        X_test, y_test = df_.iloc[:split].drop(columns=['target'], inplace=False), df_.iloc[:split]['target']\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# X_train, y_train, X_test, y_test = split_train_test(df, 600, 'simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настятельно рекомендую юзать поле enc (на каком энкодере обучалась)\n",
    "# acc ввеодить без точки, eg: r2 = 0.979 => acc = 979\n",
    "def save_model(model, acc, enc=''):\n",
    "    json_arch = model.to_json()\n",
    "    if enc:\n",
    "        model.save('./models/model_{}_{}.h5'.format(acc, enc))\n",
    "        with open('./arch/model_{}_{}.json'.format(acc, enc), 'w') as out:\n",
    "            out.write(json_arch)\n",
    "    else:\n",
    "        model.save('./models/model_{}.h5'.format(acc))\n",
    "        with open('./arch/model_{}.json'.format(acc), 'w') as out:\n",
    "            out.write(json_arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mlp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создавать все модели меняя эту функцию не самая лучшая идея, поэтому когда делаете новую модель лучше всего написать новую функцию аналагичную этой (да и вообще лучше целую секцию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_cols):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128, activation='relu', input_shape=(n_cols,), kernel_initializer='normal'))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r2 metric \n",
    "\n",
    "${R^2 = \\frac{\\sum{(y_i - \\hat{y_i})^2}}{\\sum{(y_i - \\bar{y})^2}}}$\n",
    "\n",
    "${\\hat{y_i}}$ -- предсказанное значение \n",
    "${\\bar{y}}$ -- среднее значение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeff_determination(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 128)               18560     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 249,217\n",
      "Trainable params: 249,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(df, 600, 'simple')\n",
    "\n",
    "verbose = 0 # выводить ли инфу в процессе обучения (на гитхаб лучше не заливать когда verbose > 0, а просто перетренировать с = 0)\n",
    "            # 0 - silence \n",
    "            # 1 - progress bar\n",
    "            # 2 - line per epoch \n",
    "model = create_model(X_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[coeff_determination])\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=400, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся инфа про обучение лежит в history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.954663671650985,\n",
       " 0.9532649177100376,\n",
       " 0.9533832158201407,\n",
       " 0.9565746234013484,\n",
       " 0.9501687149616835,\n",
       " 0.9580100864227896,\n",
       " 0.9519670936672147,\n",
       " 0.9602311026535607,\n",
       " 0.9531387005693246,\n",
       " 0.9597226503195055]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['val_coeff_determination'][-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "полный аналог r2 метрики определенной выше, но написсаный на numpy (r2 метрика выше возвращяет tensor, что нужно для использования в керасе, но это не удобно чтобы тестить)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_r2_score(v_true, v_pred):\n",
    "    ssres = np.sum(np.square(v_true - v_pred))\n",
    "    sstot = np.sum(np.square(v_true - np.mean(v_true)))\n",
    "    return 1 - ssres / sstot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующие три ячейки тут потому что я так хочу "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9794366494475739"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred.shape = (y_pred.shape[0], )\n",
    "np_r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32254964346765863"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test - y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 44us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.164404322306315, 0.9702041188875834]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 979, 'simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_set(enc='simple', create_model=create_model, epochs=400, verbose=0):\n",
    "    dataset = pd.read_csv('./robot_data/train_data.csv')    \n",
    "    dataset = dataset.drop(columns=['year'])\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "    X_train, y_train, X_test, y_test = split_train_test(dataset, 0, enc)\n",
    "    model = create_model(X_train.shape[1])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[coeff_determination])\n",
    "    model.fit(X_train, y_train, validation_split=0.2, epochs=epochs, verbose=verbose)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2663/2663 [==============================] - 0s 50us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.036161985565879, 0.9900444393940452]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = train_full_set()\n",
    "\n",
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try learn not only nn's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [\n",
    "    Lasso(),\n",
    "    ElasticNet(),\n",
    "    DecisionTreeRegressor(),\n",
    "    KNeighborsRegressor(),\n",
    "    GradientBoostingRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./robot_data/train_data.csv') \n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "скейлим датасет и сопоставляем значения категориям "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = simple_encode(dataset)\n",
    "\n",
    "X = dataset.values[0::, :-2:]\n",
    "y = dataset.values[0::, -1]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "X = np.c_[(X, dataset.values[0::, -2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a3d3e1c18>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_cols = [\"Regressor\", \"r2\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "acc_dict = {}\n",
    "\n",
    "for reg in regressors:\n",
    "    name = reg.__class__.__name__\n",
    "    \n",
    "    acc = cross_validate(reg, X, y, scoring=['r2', 'neg_mean_squared_error'], cv=3, n_jobs=-1)\n",
    "    if name in acc_dict:\n",
    "        acc_dict[name] += acc\n",
    "    else:\n",
    "        acc_dict[name] = acc\n",
    "\n",
    "for reg in acc_dict:\n",
    "    log_entry = pd.DataFrame([[reg, acc_dict[reg]['test_r2'].mean()]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Classifier Accuracy')\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='r2', y='Regressor', data=log, color=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train gradiend boosting regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще этот регрессор выглядит многообещающе, но я потратил на него уже три попытки и все были с отрицательным r2 -> я както неправильно подготавливаю данные. Скорее всего я где-то неправ со scaler'ом. А мб он прост не работает и не надо тратить на него время"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, n_iter_no_change=None, presort='auto',\n",
       "             random_state=None, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = X[600:], X[:600]\n",
    "y_train, y_test = y[600:], y[:600]\n",
    "\n",
    "gbr_reg = GradientBoostingRegressor()\n",
    "gbr_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = gbr_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9926041262413985"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train mlp w\\ dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dropout_model(n_cols):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, activation='relu', input_shape=(n_cols,), kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               37120     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 825,601\n",
      "Trainable params: 825,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-576db7cdde4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoeff_determination\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/keras-jupyter/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/keras-jupyter/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-jupyter/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-jupyter/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-jupyter/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(df, 600, 'simple')\n",
    "\n",
    "verbose = 0 # выводить ли инфу в процессе обучения (на гитхаб лучше не заливать когда verbose > 0, а просто перетренировать с = 0)\n",
    "            # 0 - silence \n",
    "            # 1 - progress bar\n",
    "            # 2 - line per epoch \n",
    "model = create_dropout_model(X_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[coeff_determination])\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=700, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 75us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[199.50483896891276, 0.49883066018422445]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_on_test(model, enc='simple'):\n",
    "    df = pd.read_csv('./robot_data/test_data.csv')\n",
    "    df = df.drop(columns=['year', 'target'])\n",
    "    \n",
    "    if enc == 'simple':\n",
    "        df = simple_encode(df.copy())\n",
    "        \n",
    "    elif enc == 'onehot':\n",
    "        df = oneHotEncode(df.copy())\n",
    "        \n",
    "    y_pred = model.predict(df)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submition(model, enc='simple', subm_name=None):\n",
    "    df = pd.read_csv('./robot_data/test_data.csv')\n",
    "    years = df['year']\n",
    "    \n",
    "    y_pred = get_preds_on_test(model)\n",
    "    y_pred = y_pred.reshape(1000)\n",
    "    \n",
    "    d = {'year': years.values, 'target': y_pred}\n",
    "    ans = pd.DataFrame(d)\n",
    "    ans = ans.set_index('year')\n",
    "    \n",
    "    subm_name = subm_name if subm_name else 'submission_.csv'\n",
    "    ans.to_csv(subm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submition(model, subm_name='subm_gbr_bad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
