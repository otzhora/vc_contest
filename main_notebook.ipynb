{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "import json \n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import parallel_backend\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('robot_data/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['year'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>robot_gear_compression_diff_1</th>\n",
       "      <th>weapon_robot_armour_index_2</th>\n",
       "      <th>robot_gear_compression_diff_3</th>\n",
       "      <th>robot_gear_compression_diff_4</th>\n",
       "      <th>weapon_robot_punch_right_1</th>\n",
       "      <th>robot_gear_compression_diff_6</th>\n",
       "      <th>robot_gear_compression_diff_7</th>\n",
       "      <th>robot_gear_compression_diff_8</th>\n",
       "      <th>robot_gear_compression_diff_9</th>\n",
       "      <th>robot_gear_compression_diff_10</th>\n",
       "      <th>robot_gear_circulation_1</th>\n",
       "      <th>robot_gear_circulation_2</th>\n",
       "      <th>weapon_robot_punch_left_3</th>\n",
       "      <th>weapon_robot_armour_index_5</th>\n",
       "      <th>weapon_robot_armour_index_3</th>\n",
       "      <th>robot_gear_circulation_6</th>\n",
       "      <th>weapon_robot_punch_right_4</th>\n",
       "      <th>robot_gear_circulation_8</th>\n",
       "      <th>weapon_robot_punch_right_2</th>\n",
       "      <th>weapon_robot_gun_power_4</th>\n",
       "      <th>weapon_robot_gun_power_5</th>\n",
       "      <th>robot_gear_circulation_12</th>\n",
       "      <th>robot_gear_circulation_13</th>\n",
       "      <th>robot_gear_circulation_14</th>\n",
       "      <th>robot_gear_circulation_15</th>\n",
       "      <th>weapon_robot_gun_power_3</th>\n",
       "      <th>weapon_robot_punch_left_5</th>\n",
       "      <th>robot_gear_circulation_18</th>\n",
       "      <th>robot_gear_circulation_19</th>\n",
       "      <th>robot_gear_circulation_20</th>\n",
       "      <th>robot_gear_circulation_21</th>\n",
       "      <th>weapon_robot_punch_right_5</th>\n",
       "      <th>robot_gear_coef_1</th>\n",
       "      <th>weapon_robot_gun_power_2</th>\n",
       "      <th>robot_gear_compression_1</th>\n",
       "      <th>robot_gear_compression_2</th>\n",
       "      <th>robot_gear_compression_3</th>\n",
       "      <th>weapon_robot_armour_index_4</th>\n",
       "      <th>weapon_robot_eye_laser_emission_1</th>\n",
       "      <th>robot_gear_compression_6</th>\n",
       "      <th>robot_gear_temperature_diff_1</th>\n",
       "      <th>robot_gear_temperature_diff_2</th>\n",
       "      <th>robot_gear_temperature_diff_3</th>\n",
       "      <th>weapon_robot_eye_laser_sensor_1</th>\n",
       "      <th>robot_gear_temperature_diff_5</th>\n",
       "      <th>robot_gear_temperature_diff_6</th>\n",
       "      <th>robot_gear_temperature_1</th>\n",
       "      <th>robot_gear_temperature_2</th>\n",
       "      <th>robot_gear_temperature_3</th>\n",
       "      <th>robot_gear_temperature_4</th>\n",
       "      <th>weapon_robot_eye_laser_emission_4</th>\n",
       "      <th>weapon_robot_punch_right_3</th>\n",
       "      <th>weapon_robot_gun_power_1</th>\n",
       "      <th>robot_gear_temperature_8</th>\n",
       "      <th>robot_gear_temperature_9</th>\n",
       "      <th>robot_gear_temperature_10</th>\n",
       "      <th>robot_gear_temperature_11</th>\n",
       "      <th>robot_gear_temperature_12</th>\n",
       "      <th>robot_gear_temperature_13</th>\n",
       "      <th>robot_gear_temperature_14</th>\n",
       "      <th>robotic_circuits_speed_1</th>\n",
       "      <th>robotic_circuits_speed_2</th>\n",
       "      <th>robotic_circuits_speed_3</th>\n",
       "      <th>robotic_circuits_speed_4</th>\n",
       "      <th>robotic_circuits_speed_5</th>\n",
       "      <th>robotic_circuits_speed_6</th>\n",
       "      <th>robotic_circuits_speed_12</th>\n",
       "      <th>robot_engine_speed_13</th>\n",
       "      <th>robot_engine_speed_14</th>\n",
       "      <th>robot_engine_speed_15</th>\n",
       "      <th>robot_engine_speed_16</th>\n",
       "      <th>robot_engine_circulation_2</th>\n",
       "      <th>robot_engine_circulation_3</th>\n",
       "      <th>robot_engine_circulation_4</th>\n",
       "      <th>robot_engine_circulation_6</th>\n",
       "      <th>robot_engine_circulation_7</th>\n",
       "      <th>robot_engine_circulation_8</th>\n",
       "      <th>robot_engine_ground_1</th>\n",
       "      <th>robot_engine_compression_1</th>\n",
       "      <th>robot_engine_compression_2</th>\n",
       "      <th>robot_engine_compression_3</th>\n",
       "      <th>weapon_robot_eye_laser_emission_2</th>\n",
       "      <th>robot_engine_temperature_2</th>\n",
       "      <th>robot_engine_temperature_3</th>\n",
       "      <th>robot_engine_temperature_4</th>\n",
       "      <th>robot_engine_temperature_5</th>\n",
       "      <th>robot_engine_temperature_6</th>\n",
       "      <th>weapon_robot_eye_laser_sensor_2</th>\n",
       "      <th>weapon_robot_punch_left_1</th>\n",
       "      <th>robot_engine_temperature_9</th>\n",
       "      <th>robot_engine_temperature_10</th>\n",
       "      <th>robot_engine_temperature_11</th>\n",
       "      <th>robot_engine_temperature_12</th>\n",
       "      <th>robot_engine_temperature_13</th>\n",
       "      <th>robot_engine_temperature_14</th>\n",
       "      <th>robot_engine_temperature_15</th>\n",
       "      <th>robot_engine_temperature_16</th>\n",
       "      <th>robot_engine_temperature_17</th>\n",
       "      <th>weapon_robot_eye_laser_range_2</th>\n",
       "      <th>weapon_robot_eye_laser_sensor_3</th>\n",
       "      <th>robot_engine_temperature_20</th>\n",
       "      <th>robot_engine_temperature_21</th>\n",
       "      <th>weapon_robot_eye_laser_emission_3</th>\n",
       "      <th>robot_engine_temperature_23</th>\n",
       "      <th>robot_engine_temperature_24</th>\n",
       "      <th>robot_engine_temperature_25</th>\n",
       "      <th>robot_engine_temperature_26</th>\n",
       "      <th>robot_engine_temperature_27</th>\n",
       "      <th>robot_engine_temperature_28</th>\n",
       "      <th>robot_probe_compression_diff_1</th>\n",
       "      <th>robot_probe_compression_diff_2</th>\n",
       "      <th>robot_probe_compression_diff_3</th>\n",
       "      <th>robot_probe_compression_diff_4</th>\n",
       "      <th>robot_probe_compression_diff_5</th>\n",
       "      <th>robot_probe_compression_diff_6</th>\n",
       "      <th>robot_probe_compression_diff_7</th>\n",
       "      <th>robot_probe_compression_diff_8</th>\n",
       "      <th>robot_probe_compression_diff_9</th>\n",
       "      <th>robot_probe_compression_diff_10</th>\n",
       "      <th>robot_probe_circulation_1</th>\n",
       "      <th>robot_probe_circulation_2</th>\n",
       "      <th>robot_probe_circulation_3</th>\n",
       "      <th>robot_probe_circulation_4</th>\n",
       "      <th>robot_probe_circulation_5</th>\n",
       "      <th>robot_probe_circulation_6</th>\n",
       "      <th>robot_probe_circulation_7</th>\n",
       "      <th>weapon_robot_armour_index_1</th>\n",
       "      <th>robot_probe_circulation_9</th>\n",
       "      <th>robot_probe_circulation_10</th>\n",
       "      <th>robot_probe_circulation_11</th>\n",
       "      <th>robot_probe_circulation_12</th>\n",
       "      <th>robot_probe_temperature_1</th>\n",
       "      <th>robot_probe_temperature_2</th>\n",
       "      <th>robot_probe_temperature_3</th>\n",
       "      <th>weapon_robot_eye_laser_sensor_4</th>\n",
       "      <th>robot_probe_temperature_5</th>\n",
       "      <th>robot_probe_temperature_6</th>\n",
       "      <th>robot_probe_temperature_7</th>\n",
       "      <th>robot_probe_temperature_8</th>\n",
       "      <th>robot_probe_temperature_9</th>\n",
       "      <th>weapon_robot_eye_laser_range_1</th>\n",
       "      <th>weapon_robot_punch_left_4</th>\n",
       "      <th>weapon_robot_punch_left_2</th>\n",
       "      <th>gamma_ray</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.904224</td>\n",
       "      <td>15.166449</td>\n",
       "      <td>16.411567</td>\n",
       "      <td>26.384177</td>\n",
       "      <td>18.747389</td>\n",
       "      <td>12.887911</td>\n",
       "      <td>18.491861</td>\n",
       "      <td>15.792127</td>\n",
       "      <td>7.670282</td>\n",
       "      <td>28.774358</td>\n",
       "      <td>9.909089</td>\n",
       "      <td>40.766893</td>\n",
       "      <td>22.773625</td>\n",
       "      <td>47.470404</td>\n",
       "      <td>46.674830</td>\n",
       "      <td>40.145648</td>\n",
       "      <td>10.898802</td>\n",
       "      <td>32.424613</td>\n",
       "      <td>34.632175</td>\n",
       "      <td>48.607230</td>\n",
       "      <td>1.189888</td>\n",
       "      <td>-6.183945</td>\n",
       "      <td>9.927305</td>\n",
       "      <td>1.512784</td>\n",
       "      <td>0.961644</td>\n",
       "      <td>0.905874</td>\n",
       "      <td>14.235658</td>\n",
       "      <td>40.609019</td>\n",
       "      <td>-14.548193</td>\n",
       "      <td>-13.181533</td>\n",
       "      <td>4.336607</td>\n",
       "      <td>13.949694</td>\n",
       "      <td>3.227183</td>\n",
       "      <td>25.273701</td>\n",
       "      <td>15.714763</td>\n",
       "      <td>12.167470</td>\n",
       "      <td>25.026348</td>\n",
       "      <td>41.157891</td>\n",
       "      <td>48.374696</td>\n",
       "      <td>25.147236</td>\n",
       "      <td>-6.224324</td>\n",
       "      <td>-6.924619</td>\n",
       "      <td>-3.000026</td>\n",
       "      <td>-4.751072</td>\n",
       "      <td>-30.497994</td>\n",
       "      <td>-16.426079</td>\n",
       "      <td>0.956318</td>\n",
       "      <td>1.065475</td>\n",
       "      <td>19.065808</td>\n",
       "      <td>18.473386</td>\n",
       "      <td>9.049724</td>\n",
       "      <td>2.178295</td>\n",
       "      <td>3.434999</td>\n",
       "      <td>6.635133</td>\n",
       "      <td>3.330549</td>\n",
       "      <td>1.377578</td>\n",
       "      <td>2.630183</td>\n",
       "      <td>6.660853</td>\n",
       "      <td>-44.265133</td>\n",
       "      <td>6.905124</td>\n",
       "      <td>-10.230125</td>\n",
       "      <td>14.086136</td>\n",
       "      <td>33.337482</td>\n",
       "      <td>22.082856</td>\n",
       "      <td>-2.161208</td>\n",
       "      <td>46.874595</td>\n",
       "      <td>-2.885696</td>\n",
       "      <td>3.677319</td>\n",
       "      <td>26.307353</td>\n",
       "      <td>-0.626873</td>\n",
       "      <td>-1.523823</td>\n",
       "      <td>-3.798114</td>\n",
       "      <td>5.651547</td>\n",
       "      <td>-19.019474</td>\n",
       "      <td>-0.784865</td>\n",
       "      <td>3.748141</td>\n",
       "      <td>9.118077</td>\n",
       "      <td>17.215802</td>\n",
       "      <td>-9.809502</td>\n",
       "      <td>-12.386069</td>\n",
       "      <td>3.611320</td>\n",
       "      <td>5.537812</td>\n",
       "      <td>5.397028</td>\n",
       "      <td>-12.803319</td>\n",
       "      <td>19.898864</td>\n",
       "      <td>-13.227218</td>\n",
       "      <td>9.852316</td>\n",
       "      <td>2.333973</td>\n",
       "      <td>-9.120435</td>\n",
       "      <td>3.092688</td>\n",
       "      <td>13.504738</td>\n",
       "      <td>17.023118</td>\n",
       "      <td>6.122366</td>\n",
       "      <td>13.702791</td>\n",
       "      <td>13.991228</td>\n",
       "      <td>15.650985</td>\n",
       "      <td>13.576761</td>\n",
       "      <td>20.326959</td>\n",
       "      <td>9.138219</td>\n",
       "      <td>2.245576</td>\n",
       "      <td>-0.587865</td>\n",
       "      <td>-2.595478</td>\n",
       "      <td>-3.236517</td>\n",
       "      <td>-1.838056</td>\n",
       "      <td>-4.073338</td>\n",
       "      <td>-4.074009</td>\n",
       "      <td>6.437224</td>\n",
       "      <td>10.391109</td>\n",
       "      <td>-4.669151</td>\n",
       "      <td>-23.491741</td>\n",
       "      <td>32.100224</td>\n",
       "      <td>16.940023</td>\n",
       "      <td>34.886572</td>\n",
       "      <td>9.543415</td>\n",
       "      <td>-30.723220</td>\n",
       "      <td>-13.600814</td>\n",
       "      <td>14.850165</td>\n",
       "      <td>-3.850033</td>\n",
       "      <td>-22.649161</td>\n",
       "      <td>-13.348838</td>\n",
       "      <td>1.579993</td>\n",
       "      <td>27.982304</td>\n",
       "      <td>-21.063083</td>\n",
       "      <td>-27.096981</td>\n",
       "      <td>2.654190</td>\n",
       "      <td>1.357739</td>\n",
       "      <td>1.155576</td>\n",
       "      <td>0.630935</td>\n",
       "      <td>1.623904</td>\n",
       "      <td>1.497468</td>\n",
       "      <td>3.269319</td>\n",
       "      <td>-12.954699</td>\n",
       "      <td>1.627971</td>\n",
       "      <td>-11.545831</td>\n",
       "      <td>0.528241</td>\n",
       "      <td>2.108404</td>\n",
       "      <td>1.194372</td>\n",
       "      <td>0.665295</td>\n",
       "      <td>3.078837</td>\n",
       "      <td>1.911756</td>\n",
       "      <td>1.685329</td>\n",
       "      <td>-4.555385</td>\n",
       "      <td>10.610730</td>\n",
       "      <td>moderate</td>\n",
       "      <td>20.396055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.829009</td>\n",
       "      <td>14.817183</td>\n",
       "      <td>16.353650</td>\n",
       "      <td>24.191974</td>\n",
       "      <td>22.313258</td>\n",
       "      <td>13.108823</td>\n",
       "      <td>18.451640</td>\n",
       "      <td>16.234554</td>\n",
       "      <td>9.722622</td>\n",
       "      <td>24.524993</td>\n",
       "      <td>9.314892</td>\n",
       "      <td>20.851960</td>\n",
       "      <td>40.646285</td>\n",
       "      <td>47.190100</td>\n",
       "      <td>45.877504</td>\n",
       "      <td>39.890871</td>\n",
       "      <td>12.112552</td>\n",
       "      <td>36.327078</td>\n",
       "      <td>39.331091</td>\n",
       "      <td>49.922699</td>\n",
       "      <td>-3.476083</td>\n",
       "      <td>-6.186788</td>\n",
       "      <td>9.929770</td>\n",
       "      <td>1.461015</td>\n",
       "      <td>0.918912</td>\n",
       "      <td>0.835761</td>\n",
       "      <td>13.932550</td>\n",
       "      <td>42.500281</td>\n",
       "      <td>-15.045941</td>\n",
       "      <td>-12.694736</td>\n",
       "      <td>4.309321</td>\n",
       "      <td>14.778146</td>\n",
       "      <td>4.063043</td>\n",
       "      <td>51.778912</td>\n",
       "      <td>15.755389</td>\n",
       "      <td>11.629879</td>\n",
       "      <td>25.279591</td>\n",
       "      <td>40.714217</td>\n",
       "      <td>52.230417</td>\n",
       "      <td>43.700264</td>\n",
       "      <td>-8.569177</td>\n",
       "      <td>-11.830147</td>\n",
       "      <td>-9.826176</td>\n",
       "      <td>-6.200095</td>\n",
       "      <td>-41.513246</td>\n",
       "      <td>-19.917287</td>\n",
       "      <td>-1.202393</td>\n",
       "      <td>-1.018839</td>\n",
       "      <td>13.103415</td>\n",
       "      <td>17.547540</td>\n",
       "      <td>8.198577</td>\n",
       "      <td>0.805081</td>\n",
       "      <td>3.768630</td>\n",
       "      <td>3.221765</td>\n",
       "      <td>3.396309</td>\n",
       "      <td>0.142532</td>\n",
       "      <td>1.193677</td>\n",
       "      <td>2.514940</td>\n",
       "      <td>-46.970675</td>\n",
       "      <td>-23.793164</td>\n",
       "      <td>-40.601749</td>\n",
       "      <td>9.165616</td>\n",
       "      <td>32.006672</td>\n",
       "      <td>13.626746</td>\n",
       "      <td>-2.161208</td>\n",
       "      <td>42.012949</td>\n",
       "      <td>-2.885696</td>\n",
       "      <td>-1.572028</td>\n",
       "      <td>25.027540</td>\n",
       "      <td>-0.708183</td>\n",
       "      <td>-1.265149</td>\n",
       "      <td>-3.798114</td>\n",
       "      <td>5.651547</td>\n",
       "      <td>-12.082857</td>\n",
       "      <td>-0.784865</td>\n",
       "      <td>3.540441</td>\n",
       "      <td>9.123438</td>\n",
       "      <td>38.791165</td>\n",
       "      <td>-8.268191</td>\n",
       "      <td>-27.073257</td>\n",
       "      <td>2.047121</td>\n",
       "      <td>3.167220</td>\n",
       "      <td>1.968319</td>\n",
       "      <td>-12.883382</td>\n",
       "      <td>-1.786862</td>\n",
       "      <td>-17.070863</td>\n",
       "      <td>7.609704</td>\n",
       "      <td>2.176281</td>\n",
       "      <td>5.049620</td>\n",
       "      <td>10.705052</td>\n",
       "      <td>24.745531</td>\n",
       "      <td>23.818490</td>\n",
       "      <td>7.307127</td>\n",
       "      <td>14.968410</td>\n",
       "      <td>15.358290</td>\n",
       "      <td>16.203016</td>\n",
       "      <td>15.032753</td>\n",
       "      <td>20.694469</td>\n",
       "      <td>9.524987</td>\n",
       "      <td>2.245576</td>\n",
       "      <td>-1.005475</td>\n",
       "      <td>-1.719989</td>\n",
       "      <td>-2.857909</td>\n",
       "      <td>2.387616</td>\n",
       "      <td>-2.224259</td>\n",
       "      <td>-4.074009</td>\n",
       "      <td>6.226098</td>\n",
       "      <td>10.391109</td>\n",
       "      <td>-8.844031</td>\n",
       "      <td>-23.821588</td>\n",
       "      <td>43.997752</td>\n",
       "      <td>32.109511</td>\n",
       "      <td>48.954852</td>\n",
       "      <td>7.319419</td>\n",
       "      <td>-3.106639</td>\n",
       "      <td>10.872032</td>\n",
       "      <td>2.869523</td>\n",
       "      <td>6.673960</td>\n",
       "      <td>4.463331</td>\n",
       "      <td>11.561926</td>\n",
       "      <td>1.445675</td>\n",
       "      <td>37.991359</td>\n",
       "      <td>-26.046032</td>\n",
       "      <td>-34.821047</td>\n",
       "      <td>2.440002</td>\n",
       "      <td>1.001539</td>\n",
       "      <td>1.366884</td>\n",
       "      <td>0.664777</td>\n",
       "      <td>2.060766</td>\n",
       "      <td>1.639949</td>\n",
       "      <td>3.200206</td>\n",
       "      <td>-13.394192</td>\n",
       "      <td>2.107142</td>\n",
       "      <td>2.111107</td>\n",
       "      <td>0.956559</td>\n",
       "      <td>4.116632</td>\n",
       "      <td>2.186736</td>\n",
       "      <td>0.574192</td>\n",
       "      <td>3.209856</td>\n",
       "      <td>1.546033</td>\n",
       "      <td>1.549280</td>\n",
       "      <td>-1.644398</td>\n",
       "      <td>-6.876256</td>\n",
       "      <td>moderate</td>\n",
       "      <td>18.413807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.785779</td>\n",
       "      <td>4.952384</td>\n",
       "      <td>5.133674</td>\n",
       "      <td>13.453927</td>\n",
       "      <td>-2.626315</td>\n",
       "      <td>4.615422</td>\n",
       "      <td>3.985793</td>\n",
       "      <td>1.910353</td>\n",
       "      <td>5.015227</td>\n",
       "      <td>5.903975</td>\n",
       "      <td>2.608068</td>\n",
       "      <td>-5.351592</td>\n",
       "      <td>11.667464</td>\n",
       "      <td>11.030628</td>\n",
       "      <td>10.735017</td>\n",
       "      <td>8.813446</td>\n",
       "      <td>3.985458</td>\n",
       "      <td>11.599129</td>\n",
       "      <td>12.329768</td>\n",
       "      <td>18.530218</td>\n",
       "      <td>-12.447955</td>\n",
       "      <td>-6.193296</td>\n",
       "      <td>10.136860</td>\n",
       "      <td>2.612884</td>\n",
       "      <td>0.974218</td>\n",
       "      <td>0.906767</td>\n",
       "      <td>3.026846</td>\n",
       "      <td>-20.597346</td>\n",
       "      <td>-15.103679</td>\n",
       "      <td>-13.632254</td>\n",
       "      <td>4.540783</td>\n",
       "      <td>13.691721</td>\n",
       "      <td>-1.312072</td>\n",
       "      <td>-11.678518</td>\n",
       "      <td>2.748269</td>\n",
       "      <td>2.473859</td>\n",
       "      <td>6.570108</td>\n",
       "      <td>11.126349</td>\n",
       "      <td>8.811222</td>\n",
       "      <td>-11.197065</td>\n",
       "      <td>-5.029335</td>\n",
       "      <td>-3.192665</td>\n",
       "      <td>-3.808756</td>\n",
       "      <td>-3.201062</td>\n",
       "      <td>-37.099966</td>\n",
       "      <td>-11.377644</td>\n",
       "      <td>5.747418</td>\n",
       "      <td>8.982241</td>\n",
       "      <td>10.314317</td>\n",
       "      <td>14.657907</td>\n",
       "      <td>5.416813</td>\n",
       "      <td>2.931476</td>\n",
       "      <td>2.838831</td>\n",
       "      <td>2.859771</td>\n",
       "      <td>3.371686</td>\n",
       "      <td>0.270928</td>\n",
       "      <td>2.286544</td>\n",
       "      <td>3.984483</td>\n",
       "      <td>54.851068</td>\n",
       "      <td>36.395228</td>\n",
       "      <td>11.660536</td>\n",
       "      <td>-14.076600</td>\n",
       "      <td>-13.262270</td>\n",
       "      <td>-6.245913</td>\n",
       "      <td>-2.161208</td>\n",
       "      <td>26.133265</td>\n",
       "      <td>-2.885696</td>\n",
       "      <td>8.591901</td>\n",
       "      <td>-56.866149</td>\n",
       "      <td>-0.576700</td>\n",
       "      <td>-2.464071</td>\n",
       "      <td>-3.798114</td>\n",
       "      <td>5.651547</td>\n",
       "      <td>-5.999977</td>\n",
       "      <td>-0.784865</td>\n",
       "      <td>3.893099</td>\n",
       "      <td>7.561725</td>\n",
       "      <td>-32.677466</td>\n",
       "      <td>5.215334</td>\n",
       "      <td>43.124856</td>\n",
       "      <td>3.876680</td>\n",
       "      <td>3.137828</td>\n",
       "      <td>6.943892</td>\n",
       "      <td>-7.015461</td>\n",
       "      <td>-32.619928</td>\n",
       "      <td>-28.669681</td>\n",
       "      <td>4.155563</td>\n",
       "      <td>2.106221</td>\n",
       "      <td>-11.744209</td>\n",
       "      <td>1.733197</td>\n",
       "      <td>9.542481</td>\n",
       "      <td>14.754550</td>\n",
       "      <td>5.877511</td>\n",
       "      <td>14.281714</td>\n",
       "      <td>14.262525</td>\n",
       "      <td>16.538149</td>\n",
       "      <td>15.140687</td>\n",
       "      <td>21.570683</td>\n",
       "      <td>9.784028</td>\n",
       "      <td>3.445695</td>\n",
       "      <td>3.537749</td>\n",
       "      <td>0.603608</td>\n",
       "      <td>0.596065</td>\n",
       "      <td>-3.247204</td>\n",
       "      <td>-0.374597</td>\n",
       "      <td>-0.515163</td>\n",
       "      <td>6.242417</td>\n",
       "      <td>11.315056</td>\n",
       "      <td>12.018977</td>\n",
       "      <td>7.086985</td>\n",
       "      <td>10.114534</td>\n",
       "      <td>-22.089317</td>\n",
       "      <td>-41.447510</td>\n",
       "      <td>-3.637141</td>\n",
       "      <td>-6.422254</td>\n",
       "      <td>-30.942248</td>\n",
       "      <td>-0.275010</td>\n",
       "      <td>0.924829</td>\n",
       "      <td>-79.105138</td>\n",
       "      <td>10.552648</td>\n",
       "      <td>1.051720</td>\n",
       "      <td>0.706075</td>\n",
       "      <td>15.283965</td>\n",
       "      <td>34.729677</td>\n",
       "      <td>1.949048</td>\n",
       "      <td>1.293275</td>\n",
       "      <td>0.690178</td>\n",
       "      <td>0.825595</td>\n",
       "      <td>1.672130</td>\n",
       "      <td>1.823550</td>\n",
       "      <td>3.248624</td>\n",
       "      <td>-1.142748</td>\n",
       "      <td>-1.367254</td>\n",
       "      <td>9.581747</td>\n",
       "      <td>0.956559</td>\n",
       "      <td>8.595773</td>\n",
       "      <td>5.379803</td>\n",
       "      <td>-0.817837</td>\n",
       "      <td>3.117562</td>\n",
       "      <td>2.022720</td>\n",
       "      <td>1.760883</td>\n",
       "      <td>-0.647793</td>\n",
       "      <td>-50.334014</td>\n",
       "      <td>moderate</td>\n",
       "      <td>3.181423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.280151</td>\n",
       "      <td>14.404758</td>\n",
       "      <td>16.427092</td>\n",
       "      <td>28.512888</td>\n",
       "      <td>16.987524</td>\n",
       "      <td>12.068514</td>\n",
       "      <td>17.722866</td>\n",
       "      <td>16.117127</td>\n",
       "      <td>15.303988</td>\n",
       "      <td>22.071968</td>\n",
       "      <td>14.270098</td>\n",
       "      <td>1.093200</td>\n",
       "      <td>58.364581</td>\n",
       "      <td>46.027743</td>\n",
       "      <td>45.002611</td>\n",
       "      <td>38.884433</td>\n",
       "      <td>9.715656</td>\n",
       "      <td>29.313836</td>\n",
       "      <td>31.036017</td>\n",
       "      <td>41.025708</td>\n",
       "      <td>-3.630609</td>\n",
       "      <td>-6.186887</td>\n",
       "      <td>9.919465</td>\n",
       "      <td>1.908753</td>\n",
       "      <td>1.003060</td>\n",
       "      <td>0.959420</td>\n",
       "      <td>13.780261</td>\n",
       "      <td>40.088523</td>\n",
       "      <td>-14.929992</td>\n",
       "      <td>-13.469522</td>\n",
       "      <td>4.472582</td>\n",
       "      <td>13.632497</td>\n",
       "      <td>-0.695668</td>\n",
       "      <td>2.707336</td>\n",
       "      <td>16.324431</td>\n",
       "      <td>12.635649</td>\n",
       "      <td>26.640917</td>\n",
       "      <td>46.356996</td>\n",
       "      <td>61.995778</td>\n",
       "      <td>76.256865</td>\n",
       "      <td>-5.789522</td>\n",
       "      <td>-3.514938</td>\n",
       "      <td>-2.593691</td>\n",
       "      <td>-0.230709</td>\n",
       "      <td>-27.300900</td>\n",
       "      <td>-16.881916</td>\n",
       "      <td>-0.868214</td>\n",
       "      <td>1.191245</td>\n",
       "      <td>11.983860</td>\n",
       "      <td>16.599363</td>\n",
       "      <td>7.955884</td>\n",
       "      <td>2.372484</td>\n",
       "      <td>3.151000</td>\n",
       "      <td>5.539438</td>\n",
       "      <td>2.729064</td>\n",
       "      <td>0.838315</td>\n",
       "      <td>1.937454</td>\n",
       "      <td>5.171496</td>\n",
       "      <td>-14.866713</td>\n",
       "      <td>-18.211608</td>\n",
       "      <td>11.961520</td>\n",
       "      <td>7.454739</td>\n",
       "      <td>19.540050</td>\n",
       "      <td>25.220135</td>\n",
       "      <td>-2.161208</td>\n",
       "      <td>49.101212</td>\n",
       "      <td>-2.885696</td>\n",
       "      <td>-2.073189</td>\n",
       "      <td>-1.661497</td>\n",
       "      <td>-0.620986</td>\n",
       "      <td>-1.635986</td>\n",
       "      <td>-3.798114</td>\n",
       "      <td>2.735514</td>\n",
       "      <td>-14.964221</td>\n",
       "      <td>-0.784865</td>\n",
       "      <td>3.862554</td>\n",
       "      <td>10.814516</td>\n",
       "      <td>-16.856149</td>\n",
       "      <td>-2.101216</td>\n",
       "      <td>23.395379</td>\n",
       "      <td>5.656382</td>\n",
       "      <td>2.601427</td>\n",
       "      <td>9.565148</td>\n",
       "      <td>-10.946502</td>\n",
       "      <td>-10.177850</td>\n",
       "      <td>-19.854531</td>\n",
       "      <td>7.340578</td>\n",
       "      <td>2.535338</td>\n",
       "      <td>2.442275</td>\n",
       "      <td>9.860105</td>\n",
       "      <td>27.792571</td>\n",
       "      <td>28.820591</td>\n",
       "      <td>9.336510</td>\n",
       "      <td>18.843183</td>\n",
       "      <td>20.252962</td>\n",
       "      <td>21.243331</td>\n",
       "      <td>19.959753</td>\n",
       "      <td>27.259568</td>\n",
       "      <td>11.920439</td>\n",
       "      <td>3.745677</td>\n",
       "      <td>0.438746</td>\n",
       "      <td>-1.909155</td>\n",
       "      <td>-2.870308</td>\n",
       "      <td>2.387616</td>\n",
       "      <td>-4.073338</td>\n",
       "      <td>-9.411717</td>\n",
       "      <td>8.704193</td>\n",
       "      <td>15.011473</td>\n",
       "      <td>-13.013215</td>\n",
       "      <td>-13.779791</td>\n",
       "      <td>6.782416</td>\n",
       "      <td>-11.205298</td>\n",
       "      <td>-18.188447</td>\n",
       "      <td>16.356786</td>\n",
       "      <td>-6.930751</td>\n",
       "      <td>-19.185316</td>\n",
       "      <td>12.357832</td>\n",
       "      <td>0.329683</td>\n",
       "      <td>-22.317412</td>\n",
       "      <td>47.207544</td>\n",
       "      <td>1.549449</td>\n",
       "      <td>13.951939</td>\n",
       "      <td>-21.543741</td>\n",
       "      <td>3.044392</td>\n",
       "      <td>2.772663</td>\n",
       "      <td>0.459583</td>\n",
       "      <td>0.308349</td>\n",
       "      <td>0.647667</td>\n",
       "      <td>1.612583</td>\n",
       "      <td>1.460540</td>\n",
       "      <td>3.241245</td>\n",
       "      <td>-16.083376</td>\n",
       "      <td>0.909051</td>\n",
       "      <td>10.494774</td>\n",
       "      <td>0.528241</td>\n",
       "      <td>3.799922</td>\n",
       "      <td>1.443960</td>\n",
       "      <td>0.674009</td>\n",
       "      <td>3.242115</td>\n",
       "      <td>0.904602</td>\n",
       "      <td>1.251439</td>\n",
       "      <td>-5.136062</td>\n",
       "      <td>-11.767418</td>\n",
       "      <td>moderate</td>\n",
       "      <td>17.878260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.558607</td>\n",
       "      <td>14.689644</td>\n",
       "      <td>15.482010</td>\n",
       "      <td>10.221080</td>\n",
       "      <td>20.796281</td>\n",
       "      <td>11.652582</td>\n",
       "      <td>20.178250</td>\n",
       "      <td>13.763336</td>\n",
       "      <td>-13.410173</td>\n",
       "      <td>20.173798</td>\n",
       "      <td>8.934834</td>\n",
       "      <td>8.886271</td>\n",
       "      <td>14.276316</td>\n",
       "      <td>47.738773</td>\n",
       "      <td>46.545421</td>\n",
       "      <td>40.253507</td>\n",
       "      <td>13.043598</td>\n",
       "      <td>39.223719</td>\n",
       "      <td>41.337693</td>\n",
       "      <td>52.177037</td>\n",
       "      <td>0.886695</td>\n",
       "      <td>-6.185725</td>\n",
       "      <td>9.913649</td>\n",
       "      <td>1.968300</td>\n",
       "      <td>0.968754</td>\n",
       "      <td>0.984168</td>\n",
       "      <td>14.038300</td>\n",
       "      <td>43.476943</td>\n",
       "      <td>-14.949634</td>\n",
       "      <td>-13.928786</td>\n",
       "      <td>4.035924</td>\n",
       "      <td>14.518674</td>\n",
       "      <td>3.896286</td>\n",
       "      <td>13.323275</td>\n",
       "      <td>16.527673</td>\n",
       "      <td>12.427569</td>\n",
       "      <td>27.495612</td>\n",
       "      <td>47.434892</td>\n",
       "      <td>63.206244</td>\n",
       "      <td>69.328477</td>\n",
       "      <td>-5.616545</td>\n",
       "      <td>-7.964505</td>\n",
       "      <td>-4.294930</td>\n",
       "      <td>-4.167853</td>\n",
       "      <td>3.313087</td>\n",
       "      <td>0.163748</td>\n",
       "      <td>8.131058</td>\n",
       "      <td>-0.506100</td>\n",
       "      <td>14.057789</td>\n",
       "      <td>17.410875</td>\n",
       "      <td>8.115805</td>\n",
       "      <td>3.045419</td>\n",
       "      <td>3.724217</td>\n",
       "      <td>5.970002</td>\n",
       "      <td>3.594084</td>\n",
       "      <td>1.770255</td>\n",
       "      <td>2.241501</td>\n",
       "      <td>8.886254</td>\n",
       "      <td>-4.130673</td>\n",
       "      <td>-17.609925</td>\n",
       "      <td>3.883447</td>\n",
       "      <td>0.590636</td>\n",
       "      <td>32.430530</td>\n",
       "      <td>21.673718</td>\n",
       "      <td>-2.161208</td>\n",
       "      <td>49.750244</td>\n",
       "      <td>-2.885696</td>\n",
       "      <td>-38.054783</td>\n",
       "      <td>491.708473</td>\n",
       "      <td>-0.499511</td>\n",
       "      <td>5.462909</td>\n",
       "      <td>-3.798114</td>\n",
       "      <td>-3.097597</td>\n",
       "      <td>181.898502</td>\n",
       "      <td>-0.784865</td>\n",
       "      <td>3.632604</td>\n",
       "      <td>10.778411</td>\n",
       "      <td>-44.876881</td>\n",
       "      <td>-10.799505</td>\n",
       "      <td>13.119058</td>\n",
       "      <td>3.421561</td>\n",
       "      <td>4.438767</td>\n",
       "      <td>0.932105</td>\n",
       "      <td>-11.719416</td>\n",
       "      <td>-13.688903</td>\n",
       "      <td>-18.034483</td>\n",
       "      <td>6.512499</td>\n",
       "      <td>2.599540</td>\n",
       "      <td>12.062265</td>\n",
       "      <td>12.511177</td>\n",
       "      <td>32.467110</td>\n",
       "      <td>33.230293</td>\n",
       "      <td>10.746183</td>\n",
       "      <td>21.187271</td>\n",
       "      <td>19.428493</td>\n",
       "      <td>16.426177</td>\n",
       "      <td>15.335713</td>\n",
       "      <td>19.179369</td>\n",
       "      <td>8.908758</td>\n",
       "      <td>0.145320</td>\n",
       "      <td>-3.949512</td>\n",
       "      <td>5.620597</td>\n",
       "      <td>-4.533750</td>\n",
       "      <td>15.063743</td>\n",
       "      <td>7.948006</td>\n",
       "      <td>-0.515163</td>\n",
       "      <td>8.832025</td>\n",
       "      <td>11.315056</td>\n",
       "      <td>-25.532159</td>\n",
       "      <td>-5.824100</td>\n",
       "      <td>18.771847</td>\n",
       "      <td>4.267911</td>\n",
       "      <td>11.083815</td>\n",
       "      <td>9.115312</td>\n",
       "      <td>7.577095</td>\n",
       "      <td>11.083795</td>\n",
       "      <td>14.268631</td>\n",
       "      <td>-1.015045</td>\n",
       "      <td>-10.325623</td>\n",
       "      <td>-26.416289</td>\n",
       "      <td>0.648198</td>\n",
       "      <td>-4.793615</td>\n",
       "      <td>-29.870106</td>\n",
       "      <td>8.941020</td>\n",
       "      <td>2.478081</td>\n",
       "      <td>1.751471</td>\n",
       "      <td>0.544837</td>\n",
       "      <td>0.695547</td>\n",
       "      <td>0.628867</td>\n",
       "      <td>2.028183</td>\n",
       "      <td>-1.214013</td>\n",
       "      <td>-15.500320</td>\n",
       "      <td>3.065810</td>\n",
       "      <td>-15.127357</td>\n",
       "      <td>0.813811</td>\n",
       "      <td>-43.104271</td>\n",
       "      <td>-38.797844</td>\n",
       "      <td>2.040819</td>\n",
       "      <td>3.027545</td>\n",
       "      <td>2.045397</td>\n",
       "      <td>1.550181</td>\n",
       "      <td>-2.279580</td>\n",
       "      <td>-23.324411</td>\n",
       "      <td>moderate</td>\n",
       "      <td>18.355902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   robot_gear_compression_diff_1  weapon_robot_armour_index_2  robot_gear_compression_diff_3    ...      weapon_robot_punch_left_2  gamma_ray     target\n",
       "0                      14.904224                    15.166449                      16.411567    ...                      10.610730   moderate  20.396055\n",
       "1                      14.829009                    14.817183                      16.353650    ...                      -6.876256   moderate  18.413807\n",
       "2                       5.785779                     4.952384                       5.133674    ...                     -50.334014   moderate   3.181423\n",
       "3                      14.280151                    14.404758                      16.427092    ...                     -11.767418   moderate  17.878260\n",
       "4                      14.558607                    14.689644                      15.482010    ...                     -23.324411   moderate  18.355902\n",
       "\n",
       "[5 rows x 145 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом датасете есть только 1 категориальная фича, которую мы можем кодировать или через onehot или прост сопоставить каждому значению чилсо"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode cat features with oneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def oneHotEncode(df,colNames):\n",
    "    for col in colNames:\n",
    "        if( df[col].dtype == np.dtype('object')):\n",
    "            dummies = pd.get_dummies(df[col],prefix=col)\n",
    "            df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "            #drop the encoded column\n",
    "            df.drop([col],axis = 1 , inplace=True)\n",
    "    return df\n",
    "\n",
    "# df_oh = oneHotEncode(df, ['gamma_ray'])\n",
    "# df_oh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode cat fetures with simple encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_encode(df, encode_map=None):\n",
    "    if not encode_map:\n",
    "        encode_map = {'low': 0.25, 'moderate': 0.5, 'high': 0.75, 'very high': 1}\n",
    "\n",
    "    df['gamma_ray'] = df['gamma_ray'].map(encode_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "# df_sim = simple_encode(df)\n",
    "# df_sim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем засплитить наш размеченный датасет на сет для тренировки и сет для теста модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сплит не в процентах, а в количестве семплов потому что бибу соси\n",
    "\n",
    "def split_train_test(df, split=600, enc_type='simple'):\n",
    "    if enc_type == 'simple':\n",
    "        df_ = simple_encode(df.copy())\n",
    "        X_train, y_train= df_.iloc[split:].drop(columns=['target'], inplace=False), df_.iloc[split:]['target']\n",
    "        X_test, y_test = df_.iloc[:split].drop(columns=['target'], inplace=False), df_.iloc[:split]['target']\n",
    "    elif enc_type == 'onehot':\n",
    "        df_ = oneHotEncode(df.copy(), ['gamma_ray'])\n",
    "        X_train, y_train = df_.iloc[split:].drop(columns=['target'], inplace=False), df_.iloc[split:]['target']\n",
    "        X_test, y_test = df_.iloc[:split].drop(columns=['target'], inplace=False), df_.iloc[:split]['target']\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# X_train, y_train, X_test, y_test = split_train_test(df, 600, 'simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настятельно рекомендую юзать поле enc (на каком энкодере обучалась)\n",
    "# acc ввеодить без точки, eg: r2 = 0.979 => acc = 979\n",
    "def save_model(model, acc, enc=''):\n",
    "    json_arch = model.to_json()\n",
    "    if enc:\n",
    "        model.save('./models/model_{}_{}.h5'.format(acc, enc))\n",
    "        with open('./arch/model_{}_{}.json'.format(acc, enc), 'w') as out:\n",
    "            out.write(json_arch)\n",
    "    else:\n",
    "        model.save('./models/model_{}.h5'.format(acc))\n",
    "        with open('./arch/model_{}.json'.format(acc), 'w') as out:\n",
    "            out.write(json_arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mlp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создавать все модели меняя эту функцию не самая лучшая идея, поэтому когда делаете новую модель лучше всего написать новую функцию аналагичную этой (да и вообще лучше целую секцию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_cols):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128, activation='relu', input_shape=(n_cols,), kernel_initializer='normal'))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r2 metric \n",
    "\n",
    "${R^2 = \\frac{\\sum{(y_i - \\hat{y_i})^2}}{\\sum{(y_i - \\bar{y})^2}}}$\n",
    "\n",
    "${\\hat{y_i}}$ -- предсказанное значение \n",
    "${\\bar{y}}$ -- среднее значение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeff_determination(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 128)               18560     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 249,217\n",
      "Trainable params: 249,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(df, 600, 'simple')\n",
    "\n",
    "verbose = 0 # выводить ли инфу в процессе обучения (на гитхаб лучше не заливать когда verbose > 0, а просто перетренировать с = 0)\n",
    "            # 0 - silence \n",
    "            # 1 - progress bar\n",
    "            # 2 - line per epoch \n",
    "model = create_model(X_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[coeff_determination])\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=400, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся инфа про обучение лежит в history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.954663671650985,\n",
       " 0.9532649177100376,\n",
       " 0.9533832158201407,\n",
       " 0.9565746234013484,\n",
       " 0.9501687149616835,\n",
       " 0.9580100864227896,\n",
       " 0.9519670936672147,\n",
       " 0.9602311026535607,\n",
       " 0.9531387005693246,\n",
       " 0.9597226503195055]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['val_coeff_determination'][-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "полный аналог r2 метрики определенной выше, но написсаный на numpy (r2 метрика выше возвращяет tensor, что нужно для использования в керасе, но это не удобно чтобы тестить)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_r2_score(v_true, v_pred):\n",
    "    ssres = np.sum(np.square(v_true - v_pred))\n",
    "    sstot = np.sum(np.square(v_true - np.mean(v_true)))\n",
    "    return 1 - ssres / sstot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующие три ячейки тут потому что я так хочу "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9794366494475739"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred.shape = (y_pred.shape[0], )\n",
    "np_r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32254964346765863"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test - y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 44us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.164404322306315, 0.9702041188875834]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 979, 'simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_set(enc='simple', create_model=create_model, epochs=400, verbose=0):\n",
    "    dataset = pd.read_csv('./robot_data/train_data.csv')    \n",
    "    dataset = dataset.drop(columns=['year'])\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "    X_train, y_train, X_test, y_test = split_train_test(dataset, 0, enc)\n",
    "    model = create_model(X_train.shape[1])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[coeff_determination])\n",
    "    model.fit(X_train, y_train, validation_split=0.2, epochs=epochs, verbose=verbose)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2663/2663 [==============================] - 0s 50us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.036161985565879, 0.9900444393940452]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = train_full_set()\n",
    "\n",
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try learn not only nn's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [\n",
    "    Lasso(),\n",
    "    ElasticNet(),\n",
    "    DecisionTreeRegressor(),\n",
    "    KNeighborsRegressor(),\n",
    "    GradientBoostingRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./robot_data/train_data.csv') \n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "скейлим датасет и сопоставляем значения категориям "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = simple_encode(dataset)\n",
    "\n",
    "X = dataset.values[0::, :-2:]\n",
    "y = dataset.values[0::, -1]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "X = np.c_[(X, dataset.values[0::, -2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a3d3e1c18>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_cols = [\"Regressor\", \"r2\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "acc_dict = {}\n",
    "\n",
    "for reg in regressors:\n",
    "    name = reg.__class__.__name__\n",
    "    \n",
    "    acc = cross_validate(reg, X, y, scoring=['r2', 'neg_mean_squared_error'], cv=3, n_jobs=-1)\n",
    "    if name in acc_dict:\n",
    "        acc_dict[name] += acc\n",
    "    else:\n",
    "        acc_dict[name] = acc\n",
    "\n",
    "for reg in acc_dict:\n",
    "    log_entry = pd.DataFrame([[reg, acc_dict[reg]['test_r2'].mean()]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Classifier Accuracy')\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='r2', y='Regressor', data=log, color=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train gradiend boosting regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще этот регрессор выглядит многообещающе, но я потратил на него уже три попытки и все были с отрицательным r2 -> я както неправильно подготавливаю данные. Скорее всего я где-то неправ со scaler'ом. А мб он прост не работает и не надо тратить на него время"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, n_iter_no_change=None, presort='auto',\n",
       "             random_state=None, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = X[600:], X[:600]\n",
    "y_train, y_test = y[600:], y[:600]\n",
    "\n",
    "gbr_reg = GradientBoostingRegressor()\n",
    "gbr_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = gbr_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9926041262413985"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train mlp w\\ dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dropout_model(n_cols):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, activation='relu', input_shape=(n_cols,), kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 256)               37120     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 825,601\n",
      "Trainable params: 825,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2130 samples, validate on 533 samples\n",
      "Epoch 1/700\n",
      "2130/2130 [==============================] - 1s 628us/step - loss: 258.7610 - coeff_determination: 0.2835 - val_loss: 308.4748 - val_coeff_determination: 0.7163\n",
      "Epoch 2/700\n",
      "2130/2130 [==============================] - 1s 408us/step - loss: 127.1048 - coeff_determination: 0.6569 - val_loss: 61.7363 - val_coeff_determination: 0.8190\n",
      "Epoch 3/700\n",
      "2130/2130 [==============================] - 1s 368us/step - loss: 141.6425 - coeff_determination: 0.7333 - val_loss: 33.6070 - val_coeff_determination: 0.8733\n",
      "Epoch 4/700\n",
      "2130/2130 [==============================] - 1s 343us/step - loss: 120.7417 - coeff_determination: 0.7125 - val_loss: 433.7710 - val_coeff_determination: 0.5878\n",
      "Epoch 5/700\n",
      "2130/2130 [==============================] - 1s 349us/step - loss: 84.3745 - coeff_determination: 0.8056 - val_loss: 47.0792 - val_coeff_determination: 0.8748\n",
      "Epoch 6/700\n",
      "2130/2130 [==============================] - 1s 319us/step - loss: 105.1736 - coeff_determination: 0.7984 - val_loss: 82.0759 - val_coeff_determination: 0.7524\n",
      "Epoch 7/700\n",
      "2130/2130 [==============================] - 1s 320us/step - loss: 77.8972 - coeff_determination: 0.8044 - val_loss: 86.9621 - val_coeff_determination: 0.8661\n",
      "Epoch 8/700\n",
      "2130/2130 [==============================] - 1s 384us/step - loss: 63.3063 - coeff_determination: 0.8320 - val_loss: 37.8596 - val_coeff_determination: 0.8710\n",
      "Epoch 9/700\n",
      "2130/2130 [==============================] - 1s 336us/step - loss: 70.4330 - coeff_determination: 0.8367 - val_loss: 87.3361 - val_coeff_determination: 0.8053\n",
      "Epoch 10/700\n",
      "2130/2130 [==============================] - 1s 303us/step - loss: 100.1351 - coeff_determination: 0.8172 - val_loss: 69.0266 - val_coeff_determination: 0.8657\n",
      "Epoch 11/700\n",
      "2130/2130 [==============================] - 1s 487us/step - loss: 73.5408 - coeff_determination: 0.8474 - val_loss: 75.8115 - val_coeff_determination: 0.8560\n",
      "Epoch 12/700\n",
      "2130/2130 [==============================] - 1s 390us/step - loss: 64.0142 - coeff_determination: 0.8351 - val_loss: 339.2482 - val_coeff_determination: 0.6856\n",
      "Epoch 13/700\n",
      "2130/2130 [==============================] - 1s 330us/step - loss: 132.5603 - coeff_determination: 0.7487 - val_loss: 3748.6076 - val_coeff_determination: -1.6370\n",
      "Epoch 14/700\n",
      "2130/2130 [==============================] - 1s 316us/step - loss: 994.1007 - coeff_determination: 0.4383 - val_loss: 54.5643 - val_coeff_determination: 0.8489\n",
      "Epoch 15/700\n",
      "2130/2130 [==============================] - 1s 416us/step - loss: 189.8711 - coeff_determination: 0.7715 - val_loss: 81.3328 - val_coeff_determination: 0.8210\n",
      "Epoch 16/700\n",
      "2130/2130 [==============================] - 1s 428us/step - loss: 132.1255 - coeff_determination: 0.8188 - val_loss: 55.8471 - val_coeff_determination: 0.8694\n",
      "Epoch 17/700\n",
      "2130/2130 [==============================] - 1s 430us/step - loss: 94.6906 - coeff_determination: 0.8286 - val_loss: 218.4613 - val_coeff_determination: 0.7725\n",
      "Epoch 18/700\n",
      "2130/2130 [==============================] - 1s 442us/step - loss: 244.5086 - coeff_determination: 0.7678 - val_loss: 50.5180 - val_coeff_determination: 0.8392\n",
      "Epoch 19/700\n",
      "2130/2130 [==============================] - 1s 446us/step - loss: 226.2057 - coeff_determination: 0.7734 - val_loss: 87.7975 - val_coeff_determination: 0.7944\n",
      "Epoch 20/700\n",
      "2130/2130 [==============================] - 1s 401us/step - loss: 138.2387 - coeff_determination: 0.8112 - val_loss: 157.2548 - val_coeff_determination: 0.7153\n",
      "Epoch 21/700\n",
      "2130/2130 [==============================] - 1s 411us/step - loss: 91.3756 - coeff_determination: 0.8348 - val_loss: 56.5244 - val_coeff_determination: 0.8654\n",
      "Epoch 22/700\n",
      "2130/2130 [==============================] - 1s 444us/step - loss: 56.8679 - coeff_determination: 0.8728 - val_loss: 111.1301 - val_coeff_determination: 0.7735\n",
      "Epoch 23/700\n",
      "2130/2130 [==============================] - 1s 500us/step - loss: 68.4346 - coeff_determination: 0.8784 - val_loss: 33.2439 - val_coeff_determination: 0.8773\n",
      "Epoch 24/700\n",
      "2130/2130 [==============================] - 1s 421us/step - loss: 117.2303 - coeff_determination: 0.8396 - val_loss: 241.9907 - val_coeff_determination: 0.7403\n",
      "Epoch 25/700\n",
      "2130/2130 [==============================] - 1s 398us/step - loss: 50.3849 - coeff_determination: 0.8791 - val_loss: 50.3612 - val_coeff_determination: 0.8292\n",
      "Epoch 26/700\n",
      "2130/2130 [==============================] - 1s 382us/step - loss: 33.2387 - coeff_determination: 0.9099 - val_loss: 72.3187 - val_coeff_determination: 0.8408\n",
      "Epoch 27/700\n",
      "2130/2130 [==============================] - 1s 432us/step - loss: 55.9836 - coeff_determination: 0.8920 - val_loss: 49.4510 - val_coeff_determination: 0.8295\n",
      "Epoch 28/700\n",
      "2130/2130 [==============================] - 1s 395us/step - loss: 57.0202 - coeff_determination: 0.9018 - val_loss: 141.5747 - val_coeff_determination: 0.7750\n",
      "Epoch 29/700\n",
      "2130/2130 [==============================] - 1s 263us/step - loss: 69.9059 - coeff_determination: 0.8947 - val_loss: 168.0330 - val_coeff_determination: 0.7527\n",
      "Epoch 30/700\n",
      "2130/2130 [==============================] - 1s 287us/step - loss: 53.8476 - coeff_determination: 0.9069 - val_loss: 94.9884 - val_coeff_determination: 0.8019\n",
      "Epoch 31/700\n",
      "2130/2130 [==============================] - 1s 256us/step - loss: 53.9914 - coeff_determination: 0.9048 - val_loss: 34.5199 - val_coeff_determination: 0.8787\n",
      "Epoch 32/700\n",
      "2130/2130 [==============================] - 1s 257us/step - loss: 32.8096 - coeff_determination: 0.9126 - val_loss: 56.9356 - val_coeff_determination: 0.8395\n",
      "Epoch 33/700\n",
      "2130/2130 [==============================] - 1s 256us/step - loss: 41.1043 - coeff_determination: 0.9221 - val_loss: 50.8298 - val_coeff_determination: 0.8343\n",
      "Epoch 34/700\n",
      "2130/2130 [==============================] - 1s 359us/step - loss: 36.0728 - coeff_determination: 0.9119 - val_loss: 37.1345 - val_coeff_determination: 0.8732\n",
      "Epoch 35/700\n",
      "2130/2130 [==============================] - 1s 384us/step - loss: 48.9670 - coeff_determination: 0.8917 - val_loss: 58.6051 - val_coeff_determination: 0.8662\n",
      "Epoch 36/700\n",
      "2130/2130 [==============================] - 1s 345us/step - loss: 57.5364 - coeff_determination: 0.9056 - val_loss: 47.2281 - val_coeff_determination: 0.8491\n",
      "Epoch 37/700\n",
      "2130/2130 [==============================] - 1s 318us/step - loss: 34.8594 - coeff_determination: 0.9202 - val_loss: 65.0948 - val_coeff_determination: 0.8430\n",
      "Epoch 38/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130/2130 [==============================] - 1s 322us/step - loss: 31.7933 - coeff_determination: 0.9251 - val_loss: 52.5908 - val_coeff_determination: 0.8228\n",
      "Epoch 39/700\n",
      "2130/2130 [==============================] - 1s 307us/step - loss: 23.4931 - coeff_determination: 0.9316 - val_loss: 108.4074 - val_coeff_determination: 0.8127\n",
      "Epoch 40/700\n",
      "2130/2130 [==============================] - 1s 479us/step - loss: 37.5134 - coeff_determination: 0.9240 - val_loss: 53.0647 - val_coeff_determination: 0.8645\n",
      "Epoch 41/700\n",
      "2130/2130 [==============================] - 1s 532us/step - loss: 52.2441 - coeff_determination: 0.8990 - val_loss: 177.3607 - val_coeff_determination: 0.7165\n",
      "Epoch 42/700\n",
      "2130/2130 [==============================] - 1s 494us/step - loss: 71.7929 - coeff_determination: 0.8794 - val_loss: 85.4903 - val_coeff_determination: 0.8167\n",
      "Epoch 43/700\n",
      "2130/2130 [==============================] - 1s 404us/step - loss: 53.2409 - coeff_determination: 0.8879 - val_loss: 183.0257 - val_coeff_determination: 0.7156\n",
      "Epoch 44/700\n",
      "2130/2130 [==============================] - 1s 467us/step - loss: 50.7373 - coeff_determination: 0.9017 - val_loss: 35.8277 - val_coeff_determination: 0.8906\n",
      "Epoch 45/700\n",
      "2130/2130 [==============================] - 1s 388us/step - loss: 24.2100 - coeff_determination: 0.9308 - val_loss: 68.7860 - val_coeff_determination: 0.8425\n",
      "Epoch 46/700\n",
      "2130/2130 [==============================] - 1s 341us/step - loss: 67.2932 - coeff_determination: 0.9063 - val_loss: 144.5969 - val_coeff_determination: 0.7715\n",
      "Epoch 47/700\n",
      "2130/2130 [==============================] - 1s 390us/step - loss: 40.4086 - coeff_determination: 0.9127 - val_loss: 58.1052 - val_coeff_determination: 0.8171\n",
      "Epoch 48/700\n",
      "2130/2130 [==============================] - 1s 368us/step - loss: 25.3791 - coeff_determination: 0.9260 - val_loss: 48.0156 - val_coeff_determination: 0.8593\n",
      "Epoch 49/700\n",
      "2130/2130 [==============================] - 1s 378us/step - loss: 37.8397 - coeff_determination: 0.9286 - val_loss: 38.6291 - val_coeff_determination: 0.8727\n",
      "Epoch 50/700\n",
      "2130/2130 [==============================] - 1s 309us/step - loss: 38.8867 - coeff_determination: 0.9206 - val_loss: 70.9633 - val_coeff_determination: 0.8148\n",
      "Epoch 51/700\n",
      "2130/2130 [==============================] - 1s 299us/step - loss: 28.8808 - coeff_determination: 0.9271 - val_loss: 42.2697 - val_coeff_determination: 0.8704\n",
      "Epoch 52/700\n",
      "2130/2130 [==============================] - 1s 277us/step - loss: 37.4154 - coeff_determination: 0.9297 - val_loss: 44.0986 - val_coeff_determination: 0.8537\n",
      "Epoch 53/700\n",
      "2130/2130 [==============================] - 1s 317us/step - loss: 32.7649 - coeff_determination: 0.9343 - val_loss: 37.9650 - val_coeff_determination: 0.8719\n",
      "Epoch 54/700\n",
      "2130/2130 [==============================] - 1s 330us/step - loss: 42.8033 - coeff_determination: 0.9139 - val_loss: 49.6134 - val_coeff_determination: 0.8639\n",
      "Epoch 55/700\n",
      "2130/2130 [==============================] - 1s 291us/step - loss: 30.6118 - coeff_determination: 0.9321 - val_loss: 39.6038 - val_coeff_determination: 0.8777\n",
      "Epoch 56/700\n",
      "2130/2130 [==============================] - 1s 307us/step - loss: 25.2106 - coeff_determination: 0.9319 - val_loss: 33.3486 - val_coeff_determination: 0.8798\n",
      "Epoch 57/700\n",
      "2130/2130 [==============================] - 1s 307us/step - loss: 42.0168 - coeff_determination: 0.9321 - val_loss: 44.2298 - val_coeff_determination: 0.8909\n",
      "Epoch 58/700\n",
      "2130/2130 [==============================] - 1s 271us/step - loss: 94.5096 - coeff_determination: 0.8738 - val_loss: 92.8967 - val_coeff_determination: 0.7876\n",
      "Epoch 59/700\n",
      "2130/2130 [==============================] - 1s 290us/step - loss: 55.6108 - coeff_determination: 0.9031 - val_loss: 153.7149 - val_coeff_determination: 0.7609\n",
      "Epoch 60/700\n",
      "2130/2130 [==============================] - 1s 291us/step - loss: 78.3570 - coeff_determination: 0.8819 - val_loss: 168.8273 - val_coeff_determination: 0.7232\n",
      "Epoch 61/700\n",
      "2130/2130 [==============================] - 1s 298us/step - loss: 120.2244 - coeff_determination: 0.8337 - val_loss: 167.1320 - val_coeff_determination: 0.7812\n",
      "Epoch 62/700\n",
      "2130/2130 [==============================] - 1s 338us/step - loss: 62.3521 - coeff_determination: 0.8912 - val_loss: 211.5333 - val_coeff_determination: 0.7243\n",
      "Epoch 63/700\n",
      "2130/2130 [==============================] - 1s 312us/step - loss: 54.5767 - coeff_determination: 0.8917 - val_loss: 98.6107 - val_coeff_determination: 0.8373\n",
      "Epoch 64/700\n",
      "2130/2130 [==============================] - 1s 267us/step - loss: 50.0618 - coeff_determination: 0.9127 - val_loss: 65.8891 - val_coeff_determination: 0.8450\n",
      "Epoch 65/700\n",
      "2130/2130 [==============================] - 1s 260us/step - loss: 42.3070 - coeff_determination: 0.9239 - val_loss: 114.2945 - val_coeff_determination: 0.8206\n",
      "Epoch 66/700\n",
      "2130/2130 [==============================] - 1s 281us/step - loss: 40.4457 - coeff_determination: 0.9083 - val_loss: 60.4275 - val_coeff_determination: 0.8373\n",
      "Epoch 67/700\n",
      "2130/2130 [==============================] - 1s 313us/step - loss: 141.4986 - coeff_determination: 0.8297 - val_loss: 439.6906 - val_coeff_determination: 0.4826\n",
      "Epoch 68/700\n",
      "2130/2130 [==============================] - 1s 329us/step - loss: 158.7959 - coeff_determination: 0.7790 - val_loss: 42.6824 - val_coeff_determination: 0.8812\n",
      "Epoch 69/700\n",
      "2130/2130 [==============================] - 1s 368us/step - loss: 61.9998 - coeff_determination: 0.8781 - val_loss: 68.0532 - val_coeff_determination: 0.8144\n",
      "Epoch 70/700\n",
      "2130/2130 [==============================] - 1s 379us/step - loss: 64.9456 - coeff_determination: 0.8931 - val_loss: 149.4661 - val_coeff_determination: 0.7451\n",
      "Epoch 71/700\n",
      "2130/2130 [==============================] - 1s 320us/step - loss: 65.9117 - coeff_determination: 0.8803 - val_loss: 192.6364 - val_coeff_determination: 0.6900\n",
      "Epoch 72/700\n",
      "2130/2130 [==============================] - 1s 272us/step - loss: 210.6575 - coeff_determination: 0.7620 - val_loss: 45.6679 - val_coeff_determination: 0.8541\n",
      "Epoch 73/700\n",
      "2130/2130 [==============================] - 1s 292us/step - loss: 50.7735 - coeff_determination: 0.9045 - val_loss: 106.7193 - val_coeff_determination: 0.7965\n",
      "Epoch 74/700\n",
      "2130/2130 [==============================] - 1s 339us/step - loss: 91.1498 - coeff_determination: 0.8721 - val_loss: 162.2025 - val_coeff_determination: 0.7375\n",
      "Epoch 75/700\n",
      "2130/2130 [==============================] - 1s 367us/step - loss: 51.4767 - coeff_determination: 0.8988 - val_loss: 41.4594 - val_coeff_determination: 0.8492\n",
      "Epoch 76/700\n",
      "2130/2130 [==============================] - 1s 423us/step - loss: 29.6191 - coeff_determination: 0.9253 - val_loss: 42.5991 - val_coeff_determination: 0.8622\n",
      "Epoch 77/700\n",
      "2130/2130 [==============================] - 1s 407us/step - loss: 90.2328 - coeff_determination: 0.8675 - val_loss: 96.0413 - val_coeff_determination: 0.8051\n",
      "Epoch 78/700\n",
      "2130/2130 [==============================] - 1s 404us/step - loss: 66.1629 - coeff_determination: 0.8678 - val_loss: 147.7610 - val_coeff_determination: 0.7886\n",
      "Epoch 79/700\n",
      "2130/2130 [==============================] - 1s 292us/step - loss: 37.6881 - coeff_determination: 0.9218 - val_loss: 83.9488 - val_coeff_determination: 0.8211\n",
      "Epoch 80/700\n",
      "2130/2130 [==============================] - 1s 315us/step - loss: 80.8667 - coeff_determination: 0.8705 - val_loss: 167.3242 - val_coeff_determination: 0.7477\n",
      "Epoch 81/700\n",
      "2130/2130 [==============================] - 1s 318us/step - loss: 64.5168 - coeff_determination: 0.8994 - val_loss: 98.8665 - val_coeff_determination: 0.8205\n",
      "Epoch 82/700\n",
      "2130/2130 [==============================] - 1s 366us/step - loss: 53.5867 - coeff_determination: 0.9107 - val_loss: 91.6976 - val_coeff_determination: 0.8355\n",
      "Epoch 83/700\n",
      "2130/2130 [==============================] - 1s 451us/step - loss: 76.7918 - coeff_determination: 0.8799 - val_loss: 49.9007 - val_coeff_determination: 0.8589\n",
      "Epoch 84/700\n",
      "2130/2130 [==============================] - 1s 423us/step - loss: 169.1149 - coeff_determination: 0.8022 - val_loss: 343.3734 - val_coeff_determination: 0.6619\n",
      "Epoch 85/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130/2130 [==============================] - 1s 310us/step - loss: 156.0529 - coeff_determination: 0.7807 - val_loss: 92.0655 - val_coeff_determination: 0.7610\n",
      "Epoch 86/700\n",
      "2130/2130 [==============================] - 1s 437us/step - loss: 59.1017 - coeff_determination: 0.8985 - val_loss: 278.6436 - val_coeff_determination: 0.6573\n",
      "Epoch 87/700\n",
      "2130/2130 [==============================] - 1s 362us/step - loss: 176.3678 - coeff_determination: 0.7993 - val_loss: 295.3870 - val_coeff_determination: 0.5838\n",
      "Epoch 88/700\n",
      "2130/2130 [==============================] - 1s 314us/step - loss: 85.5621 - coeff_determination: 0.8407 - val_loss: 87.8019 - val_coeff_determination: 0.8490\n",
      "Epoch 89/700\n",
      "2130/2130 [==============================] - 1s 299us/step - loss: 156.0815 - coeff_determination: 0.8117 - val_loss: 136.5320 - val_coeff_determination: 0.7401\n",
      "Epoch 90/700\n",
      "2130/2130 [==============================] - 1s 293us/step - loss: 194.6351 - coeff_determination: 0.8268 - val_loss: 54.0251 - val_coeff_determination: 0.8311\n",
      "Epoch 91/700\n",
      "2130/2130 [==============================] - 1s 310us/step - loss: 86.7446 - coeff_determination: 0.8542 - val_loss: 155.9578 - val_coeff_determination: 0.7698\n",
      "Epoch 92/700\n",
      "2130/2130 [==============================] - 1s 326us/step - loss: 78.2102 - coeff_determination: 0.8786 - val_loss: 99.6271 - val_coeff_determination: 0.8204\n",
      "Epoch 93/700\n",
      "2130/2130 [==============================] - 1s 300us/step - loss: 58.6808 - coeff_determination: 0.8937 - val_loss: 515.0501 - val_coeff_determination: 0.5197\n",
      "Epoch 94/700\n",
      "2130/2130 [==============================] - 1s 278us/step - loss: 117.8402 - coeff_determination: 0.8410 - val_loss: 52.0903 - val_coeff_determination: 0.8002\n",
      "Epoch 95/700\n",
      "2130/2130 [==============================] - 1s 284us/step - loss: 64.6773 - coeff_determination: 0.8721 - val_loss: 43.3827 - val_coeff_determination: 0.8510\n",
      "Epoch 96/700\n",
      "2130/2130 [==============================] - 1s 351us/step - loss: 43.5921 - coeff_determination: 0.9069 - val_loss: 130.5247 - val_coeff_determination: 0.7923\n",
      "Epoch 97/700\n",
      "2130/2130 [==============================] - 1s 290us/step - loss: 60.2656 - coeff_determination: 0.9024 - val_loss: 44.9254 - val_coeff_determination: 0.8393\n",
      "Epoch 98/700\n",
      "2130/2130 [==============================] - 1s 290us/step - loss: 63.8939 - coeff_determination: 0.8999 - val_loss: 217.2868 - val_coeff_determination: 0.7368\n",
      "Epoch 99/700\n",
      "2130/2130 [==============================] - 1s 273us/step - loss: 38.7590 - coeff_determination: 0.9158 - val_loss: 124.8436 - val_coeff_determination: 0.8060\n",
      "Epoch 100/700\n",
      "2130/2130 [==============================] - 1s 288us/step - loss: 45.2584 - coeff_determination: 0.9101 - val_loss: 71.5929 - val_coeff_determination: 0.7642\n",
      "Epoch 101/700\n",
      "2130/2130 [==============================] - 1s 289us/step - loss: 47.2564 - coeff_determination: 0.9048 - val_loss: 76.7402 - val_coeff_determination: 0.7642\n",
      "Epoch 102/700\n",
      "2130/2130 [==============================] - ETA: 0s - loss: 101.1547 - coeff_determination: 0.869 - 1s 298us/step - loss: 109.5502 - coeff_determination: 0.8673 - val_loss: 98.8479 - val_coeff_determination: 0.7826\n",
      "Epoch 103/700\n",
      "2130/2130 [==============================] - 1s 288us/step - loss: 94.6493 - coeff_determination: 0.8789 - val_loss: 103.9907 - val_coeff_determination: 0.8018\n",
      "Epoch 104/700\n",
      "2130/2130 [==============================] - 1s 257us/step - loss: 73.8840 - coeff_determination: 0.8926 - val_loss: 80.9884 - val_coeff_determination: 0.7829\n",
      "Epoch 105/700\n",
      "2130/2130 [==============================] - 1s 258us/step - loss: 29.8250 - coeff_determination: 0.9318 - val_loss: 114.4969 - val_coeff_determination: 0.7762\n",
      "Epoch 106/700\n",
      "2130/2130 [==============================] - 1s 291us/step - loss: 46.7325 - coeff_determination: 0.9150 - val_loss: 143.6371 - val_coeff_determination: 0.7251\n",
      "Epoch 107/700\n",
      "2130/2130 [==============================] - 1s 272us/step - loss: 33.9551 - coeff_determination: 0.9288 - val_loss: 106.8935 - val_coeff_determination: 0.7793\n",
      "Epoch 108/700\n",
      "2130/2130 [==============================] - 1s 263us/step - loss: 28.9950 - coeff_determination: 0.9350 - val_loss: 46.2093 - val_coeff_determination: 0.8364\n",
      "Epoch 109/700\n",
      "2130/2130 [==============================] - 1s 278us/step - loss: 30.2088 - coeff_determination: 0.9253 - val_loss: 59.5579 - val_coeff_determination: 0.7860\n",
      "Epoch 110/700\n",
      "2130/2130 [==============================] - 1s 263us/step - loss: 40.7502 - coeff_determination: 0.9227 - val_loss: 45.4337 - val_coeff_determination: 0.8268\n",
      "Epoch 111/700\n",
      "2130/2130 [==============================] - 1s 256us/step - loss: 24.9540 - coeff_determination: 0.9383 - val_loss: 72.6227 - val_coeff_determination: 0.7825\n",
      "Epoch 112/700\n",
      "2130/2130 [==============================] - 1s 270us/step - loss: 35.1324 - coeff_determination: 0.9199 - val_loss: 61.7397 - val_coeff_determination: 0.8058\n",
      "Epoch 113/700\n",
      "2130/2130 [==============================] - 1s 323us/step - loss: 39.9614 - coeff_determination: 0.9244 - val_loss: 81.2244 - val_coeff_determination: 0.7424\n",
      "Epoch 114/700\n",
      "2130/2130 [==============================] - 1s 318us/step - loss: 23.5586 - coeff_determination: 0.9373 - val_loss: 63.8413 - val_coeff_determination: 0.7909\n",
      "Epoch 115/700\n",
      "2130/2130 [==============================] - 1s 287us/step - loss: 30.7901 - coeff_determination: 0.9305 - val_loss: 94.2913 - val_coeff_determination: 0.7290\n",
      "Epoch 116/700\n",
      "2130/2130 [==============================] - 1s 404us/step - loss: 149.1730 - coeff_determination: 0.7994 - val_loss: 252.3749 - val_coeff_determination: 0.6833\n",
      "Epoch 117/700\n",
      "2130/2130 [==============================] - 1s 310us/step - loss: 57.9430 - coeff_determination: 0.8911 - val_loss: 57.3674 - val_coeff_determination: 0.7937\n",
      "Epoch 118/700\n",
      "2130/2130 [==============================] - 1s 344us/step - loss: 35.1420 - coeff_determination: 0.9273 - val_loss: 54.0509 - val_coeff_determination: 0.8208\n",
      "Epoch 119/700\n",
      "2130/2130 [==============================] - 1s 297us/step - loss: 96.6114 - coeff_determination: 0.8642 - val_loss: 216.5318 - val_coeff_determination: 0.6033\n",
      "Epoch 120/700\n",
      "2130/2130 [==============================] - 1s 269us/step - loss: 59.9928 - coeff_determination: 0.9067 - val_loss: 201.9894 - val_coeff_determination: 0.7137\n",
      "Epoch 121/700\n",
      "2130/2130 [==============================] - 1s 264us/step - loss: 38.5378 - coeff_determination: 0.9210 - val_loss: 63.7444 - val_coeff_determination: 0.7967\n",
      "Epoch 122/700\n",
      "2130/2130 [==============================] - 1s 262us/step - loss: 33.7073 - coeff_determination: 0.9275 - val_loss: 67.1274 - val_coeff_determination: 0.8162\n",
      "Epoch 123/700\n",
      "2130/2130 [==============================] - 1s 269us/step - loss: 37.4053 - coeff_determination: 0.9264 - val_loss: 160.0240 - val_coeff_determination: 0.6853\n",
      "Epoch 124/700\n",
      "2130/2130 [==============================] - 1s 261us/step - loss: 35.3880 - coeff_determination: 0.9362 - val_loss: 56.6957 - val_coeff_determination: 0.7934\n",
      "Epoch 125/700\n",
      "2130/2130 [==============================] - 1s 261us/step - loss: 16.8084 - coeff_determination: 0.9448 - val_loss: 86.5593 - val_coeff_determination: 0.7508\n",
      "Epoch 126/700\n",
      "2130/2130 [==============================] - 1s 285us/step - loss: 36.0591 - coeff_determination: 0.9190 - val_loss: 175.7170 - val_coeff_determination: 0.7244\n",
      "Epoch 127/700\n",
      "2130/2130 [==============================] - 1s 262us/step - loss: 26.7200 - coeff_determination: 0.9314 - val_loss: 114.3466 - val_coeff_determination: 0.7339\n",
      "Epoch 128/700\n",
      "2130/2130 [==============================] - 1s 284us/step - loss: 72.0181 - coeff_determination: 0.9016 - val_loss: 221.1019 - val_coeff_determination: 0.7273\n",
      "Epoch 129/700\n",
      "2130/2130 [==============================] - 1s 328us/step - loss: 49.6389 - coeff_determination: 0.8995 - val_loss: 71.9742 - val_coeff_determination: 0.7657\n",
      "Epoch 130/700\n",
      "2130/2130 [==============================] - 1s 334us/step - loss: 140.8699 - coeff_determination: 0.7976 - val_loss: 194.6708 - val_coeff_determination: 0.6991\n",
      "Epoch 131/700\n",
      "2130/2130 [==============================] - 1s 295us/step - loss: 60.4626 - coeff_determination: 0.9120 - val_loss: 67.5118 - val_coeff_determination: 0.7895\n",
      "Epoch 132/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130/2130 [==============================] - 1s 260us/step - loss: 105.3163 - coeff_determination: 0.8579 - val_loss: 139.2044 - val_coeff_determination: 0.7497\n",
      "Epoch 133/700\n",
      "2130/2130 [==============================] - 1s 265us/step - loss: 62.3720 - coeff_determination: 0.8781 - val_loss: 68.8524 - val_coeff_determination: 0.8109\n",
      "Epoch 134/700\n",
      "2130/2130 [==============================] - 1s 266us/step - loss: 30.4748 - coeff_determination: 0.9313 - val_loss: 64.2590 - val_coeff_determination: 0.7884\n",
      "Epoch 135/700\n",
      "2130/2130 [==============================] - 1s 265us/step - loss: 32.0720 - coeff_determination: 0.9372 - val_loss: 66.0896 - val_coeff_determination: 0.7664\n",
      "Epoch 136/700\n",
      "2130/2130 [==============================] - 1s 263us/step - loss: 22.6153 - coeff_determination: 0.9401 - val_loss: 77.3976 - val_coeff_determination: 0.7661\n",
      "Epoch 137/700\n",
      "2130/2130 [==============================] - 1s 264us/step - loss: 38.8775 - coeff_determination: 0.9358 - val_loss: 126.4362 - val_coeff_determination: 0.7752\n",
      "Epoch 138/700\n",
      "2130/2130 [==============================] - 1s 251us/step - loss: 20.7201 - coeff_determination: 0.9447 - val_loss: 56.5806 - val_coeff_determination: 0.7941\n",
      "Epoch 139/700\n",
      "2130/2130 [==============================] - 1s 258us/step - loss: 22.2139 - coeff_determination: 0.9450 - val_loss: 65.9607 - val_coeff_determination: 0.7743\n",
      "Epoch 140/700\n",
      "2130/2130 [==============================] - 1s 266us/step - loss: 37.6636 - coeff_determination: 0.9238 - val_loss: 205.5778 - val_coeff_determination: 0.6535\n",
      "Epoch 141/700\n",
      "2130/2130 [==============================] - 1s 267us/step - loss: 36.1389 - coeff_determination: 0.9351 - val_loss: 73.3625 - val_coeff_determination: 0.7339\n",
      "Epoch 142/700\n",
      "2130/2130 [==============================] - 1s 261us/step - loss: 38.6626 - coeff_determination: 0.9250 - val_loss: 160.6231 - val_coeff_determination: 0.6978\n",
      "Epoch 143/700\n",
      "2130/2130 [==============================] - 1s 333us/step - loss: 62.4012 - coeff_determination: 0.9087 - val_loss: 107.0117 - val_coeff_determination: 0.7870\n",
      "Epoch 144/700\n",
      "2130/2130 [==============================] - 1s 340us/step - loss: 38.2610 - coeff_determination: 0.9162 - val_loss: 67.5803 - val_coeff_determination: 0.7828\n",
      "Epoch 145/700\n",
      "2130/2130 [==============================] - 1s 311us/step - loss: 29.7812 - coeff_determination: 0.9295 - val_loss: 60.8566 - val_coeff_determination: 0.8100\n",
      "Epoch 146/700\n",
      "2130/2130 [==============================] - 1s 400us/step - loss: 34.0875 - coeff_determination: 0.9362 - val_loss: 71.1843 - val_coeff_determination: 0.7854\n",
      "Epoch 147/700\n",
      "2130/2130 [==============================] - 1s 369us/step - loss: 29.6485 - coeff_determination: 0.9379 - val_loss: 68.7775 - val_coeff_determination: 0.7759\n",
      "Epoch 148/700\n",
      "2130/2130 [==============================] - 1s 260us/step - loss: 28.5845 - coeff_determination: 0.9342 - val_loss: 88.8917 - val_coeff_determination: 0.7735\n",
      "Epoch 149/700\n",
      "2130/2130 [==============================] - 1s 265us/step - loss: 44.2145 - coeff_determination: 0.9169 - val_loss: 181.6306 - val_coeff_determination: 0.6740\n",
      "Epoch 150/700\n",
      "2130/2130 [==============================] - 1s 262us/step - loss: 34.3215 - coeff_determination: 0.9302 - val_loss: 68.4608 - val_coeff_determination: 0.7559\n",
      "Epoch 151/700\n",
      "2130/2130 [==============================] - 1s 257us/step - loss: 47.9097 - coeff_determination: 0.9201 - val_loss: 73.1570 - val_coeff_determination: 0.7470\n",
      "Epoch 152/700\n",
      "2130/2130 [==============================] - 1s 264us/step - loss: 27.0504 - coeff_determination: 0.9464 - val_loss: 138.7027 - val_coeff_determination: 0.7137\n",
      "Epoch 153/700\n",
      "2130/2130 [==============================] - 1s 264us/step - loss: 37.0763 - coeff_determination: 0.9284 - val_loss: 83.2603 - val_coeff_determination: 0.7751\n",
      "Epoch 154/700\n",
      "2130/2130 [==============================] - 1s 251us/step - loss: 41.1437 - coeff_determination: 0.9193 - val_loss: 51.7007 - val_coeff_determination: 0.8161\n",
      "Epoch 155/700\n",
      "2130/2130 [==============================] - 1s 259us/step - loss: 27.9385 - coeff_determination: 0.9460 - val_loss: 146.2675 - val_coeff_determination: 0.7218\n",
      "Epoch 156/700\n",
      "2130/2130 [==============================] - 1s 253us/step - loss: 31.8537 - coeff_determination: 0.9203 - val_loss: 77.2588 - val_coeff_determination: 0.7448\n",
      "Epoch 157/700\n",
      "2130/2130 [==============================] - 1s 250us/step - loss: 23.5318 - coeff_determination: 0.9368 - val_loss: 81.9921 - val_coeff_determination: 0.7068\n",
      "Epoch 158/700\n",
      "2130/2130 [==============================] - 1s 255us/step - loss: 31.8813 - coeff_determination: 0.9344 - val_loss: 79.8164 - val_coeff_determination: 0.7825\n",
      "Epoch 159/700\n",
      "2130/2130 [==============================] - 1s 252us/step - loss: 32.1420 - coeff_determination: 0.9390 - val_loss: 88.2456 - val_coeff_determination: 0.7132\n",
      "Epoch 160/700\n",
      "2130/2130 [==============================] - 1s 261us/step - loss: 36.1804 - coeff_determination: 0.9289 - val_loss: 64.3410 - val_coeff_determination: 0.7677\n",
      "Epoch 161/700\n",
      "2130/2130 [==============================] - 1s 256us/step - loss: 37.9208 - coeff_determination: 0.9162 - val_loss: 136.5942 - val_coeff_determination: 0.6595\n",
      "Epoch 162/700\n",
      "2130/2130 [==============================] - 1s 305us/step - loss: 36.9539 - coeff_determination: 0.9426 - val_loss: 60.0273 - val_coeff_determination: 0.7765\n",
      "Epoch 163/700\n",
      "2130/2130 [==============================] - 1s 344us/step - loss: 60.8894 - coeff_determination: 0.9090 - val_loss: 108.3637 - val_coeff_determination: 0.7284\n",
      "Epoch 164/700\n",
      "2130/2130 [==============================] - 1s 275us/step - loss: 46.7807 - coeff_determination: 0.9133 - val_loss: 129.2935 - val_coeff_determination: 0.7179\n",
      "Epoch 165/700\n",
      "2130/2130 [==============================] - 1s 267us/step - loss: 58.1688 - coeff_determination: 0.9171 - val_loss: 80.6635 - val_coeff_determination: 0.7227\n",
      "Epoch 166/700\n",
      "2130/2130 [==============================] - 1s 260us/step - loss: 54.2410 - coeff_determination: 0.8990 - val_loss: 70.4648 - val_coeff_determination: 0.7855\n",
      "Epoch 167/700\n",
      "2130/2130 [==============================] - 1s 253us/step - loss: 54.3265 - coeff_determination: 0.9078 - val_loss: 189.2426 - val_coeff_determination: 0.6661\n",
      "Epoch 168/700\n",
      "2130/2130 [==============================] - 1s 255us/step - loss: 56.9367 - coeff_determination: 0.9000 - val_loss: 61.8443 - val_coeff_determination: 0.7771\n",
      "Epoch 169/700\n",
      "2130/2130 [==============================] - 1s 260us/step - loss: 218.4053 - coeff_determination: 0.7460 - val_loss: 494.2095 - val_coeff_determination: 0.2949\n",
      "Epoch 170/700\n",
      "2130/2130 [==============================] - 1s 263us/step - loss: 135.5782 - coeff_determination: 0.6958 - val_loss: 136.8980 - val_coeff_determination: 0.6501\n",
      "Epoch 171/700\n",
      "2130/2130 [==============================] - 1s 268us/step - loss: 121.3812 - coeff_determination: 0.8178 - val_loss: 87.0081 - val_coeff_determination: 0.7286\n",
      "Epoch 172/700\n",
      "2130/2130 [==============================] - 1s 263us/step - loss: 56.2612 - coeff_determination: 0.8961 - val_loss: 151.1869 - val_coeff_determination: 0.6964\n",
      "Epoch 173/700\n",
      "2130/2130 [==============================] - 1s 256us/step - loss: 63.2597 - coeff_determination: 0.8644 - val_loss: 370.7306 - val_coeff_determination: 0.4433\n",
      "Epoch 174/700\n",
      "2130/2130 [==============================] - 1s 288us/step - loss: 105.2626 - coeff_determination: 0.8536 - val_loss: 118.3099 - val_coeff_determination: 0.6789\n",
      "Epoch 175/700\n",
      "2130/2130 [==============================] - 1s 285us/step - loss: 186.8271 - coeff_determination: 0.7252 - val_loss: 431.4043 - val_coeff_determination: 0.4013\n",
      "Epoch 176/700\n",
      "2130/2130 [==============================] - 1s 332us/step - loss: 115.8106 - coeff_determination: 0.8113 - val_loss: 100.5971 - val_coeff_determination: 0.6727\n",
      "Epoch 177/700\n",
      "2130/2130 [==============================] - 1s 270us/step - loss: 47.3913 - coeff_determination: 0.9019 - val_loss: 64.5579 - val_coeff_determination: 0.7679\n",
      "Epoch 178/700\n",
      "2130/2130 [==============================] - 1s 265us/step - loss: 117.1554 - coeff_determination: 0.8151 - val_loss: 186.7178 - val_coeff_determination: 0.6662\n",
      "Epoch 179/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130/2130 [==============================] - 1s 284us/step - loss: 49.4516 - coeff_determination: 0.8981 - val_loss: 127.1939 - val_coeff_determination: 0.6350\n",
      "Epoch 180/700\n",
      "2130/2130 [==============================] - 1s 287us/step - loss: 38.4693 - coeff_determination: 0.9233 - val_loss: 81.4461 - val_coeff_determination: 0.7331\n",
      "Epoch 181/700\n",
      "2130/2130 [==============================] - 1s 292us/step - loss: 67.4075 - coeff_determination: 0.9011 - val_loss: 77.6673 - val_coeff_determination: 0.7597\n",
      "Epoch 182/700\n",
      "2130/2130 [==============================] - 1s 262us/step - loss: 38.5797 - coeff_determination: 0.9133 - val_loss: 86.5514 - val_coeff_determination: 0.6919\n",
      "Epoch 183/700\n",
      "2130/2130 [==============================] - 1s 262us/step - loss: 67.6641 - coeff_determination: 0.8982 - val_loss: 79.5036 - val_coeff_determination: 0.7040\n",
      "Epoch 184/700\n",
      "2130/2130 [==============================] - 1s 262us/step - loss: 66.9188 - coeff_determination: 0.8961 - val_loss: 162.3741 - val_coeff_determination: 0.6546\n",
      "Epoch 185/700\n",
      "2130/2130 [==============================] - 1s 259us/step - loss: 38.6222 - coeff_determination: 0.9261 - val_loss: 63.5136 - val_coeff_determination: 0.7623\n",
      "Epoch 186/700\n",
      "2130/2130 [==============================] - 1s 264us/step - loss: 34.8625 - coeff_determination: 0.9349 - val_loss: 92.1662 - val_coeff_determination: 0.6984\n",
      "Epoch 187/700\n",
      "2130/2130 [==============================] - 1s 265us/step - loss: 40.0356 - coeff_determination: 0.9325 - val_loss: 89.7399 - val_coeff_determination: 0.7228\n",
      "Epoch 188/700\n",
      "2130/2130 [==============================] - 1s 263us/step - loss: 48.7162 - coeff_determination: 0.9021 - val_loss: 330.7982 - val_coeff_determination: 0.5705\n",
      "Epoch 189/700\n",
      "2130/2130 [==============================] - 1s 262us/step - loss: 107.8746 - coeff_determination: 0.8377 - val_loss: 166.8451 - val_coeff_determination: 0.6809\n",
      "Epoch 190/700\n",
      "2130/2130 [==============================] - 1s 260us/step - loss: 92.5719 - coeff_determination: 0.8688 - val_loss: 385.6090 - val_coeff_determination: 0.4250\n",
      "Epoch 191/700\n",
      "2130/2130 [==============================] - 1s 257us/step - loss: 68.6494 - coeff_determination: 0.8817 - val_loss: 116.7307 - val_coeff_determination: 0.6888\n",
      "Epoch 192/700\n",
      "2130/2130 [==============================] - 1s 248us/step - loss: 27.9149 - coeff_determination: 0.9342 - val_loss: 108.0442 - val_coeff_determination: 0.7176\n",
      "Epoch 193/700\n",
      "2130/2130 [==============================] - 1s 253us/step - loss: 27.9866 - coeff_determination: 0.9380 - val_loss: 103.1233 - val_coeff_determination: 0.7156\n",
      "Epoch 194/700\n",
      "2130/2130 [==============================] - 1s 259us/step - loss: 23.8842 - coeff_determination: 0.9441 - val_loss: 73.4225 - val_coeff_determination: 0.7325\n",
      "Epoch 195/700\n",
      "2130/2130 [==============================] - 1s 258us/step - loss: 33.5049 - coeff_determination: 0.9354 - val_loss: 101.2418 - val_coeff_determination: 0.6902\n",
      "Epoch 196/700\n",
      "2130/2130 [==============================] - 1s 258us/step - loss: 20.4246 - coeff_determination: 0.9412 - val_loss: 110.0746 - val_coeff_determination: 0.6805\n",
      "Epoch 197/700\n",
      "2130/2130 [==============================] - 1s 274us/step - loss: 34.5638 - coeff_determination: 0.9367 - val_loss: 124.8553 - val_coeff_determination: 0.6725\n",
      "Epoch 198/700\n",
      "2130/2130 [==============================] - 1s 288us/step - loss: 22.3514 - coeff_determination: 0.9473 - val_loss: 68.9690 - val_coeff_determination: 0.7496\n",
      "Epoch 199/700\n",
      "2130/2130 [==============================] - 1s 279us/step - loss: 22.0672 - coeff_determination: 0.9478 - val_loss: 135.3699 - val_coeff_determination: 0.6831\n",
      "Epoch 200/700\n",
      "2130/2130 [==============================] - 1s 258us/step - loss: 56.3810 - coeff_determination: 0.9090 - val_loss: 118.5431 - val_coeff_determination: 0.6993\n",
      "Epoch 201/700\n",
      "2130/2130 [==============================] - 1s 261us/step - loss: 33.1209 - coeff_determination: 0.9224 - val_loss: 84.9293 - val_coeff_determination: 0.7279\n",
      "Epoch 202/700\n",
      "2130/2130 [==============================] - 1s 257us/step - loss: 101.7428 - coeff_determination: 0.8707 - val_loss: 386.6439 - val_coeff_determination: 0.4380\n",
      "Epoch 203/700\n",
      "2130/2130 [==============================] - 1s 257us/step - loss: 89.9246 - coeff_determination: 0.8909 - val_loss: 203.4763 - val_coeff_determination: 0.6134\n",
      "Epoch 204/700\n",
      "2130/2130 [==============================] - 1s 249us/step - loss: 22.1985 - coeff_determination: 0.9409 - val_loss: 91.4058 - val_coeff_determination: 0.6857\n",
      "Epoch 205/700\n",
      "2130/2130 [==============================] - 1s 254us/step - loss: 32.0169 - coeff_determination: 0.9323 - val_loss: 214.0524 - val_coeff_determination: 0.5689\n",
      "Epoch 206/700\n",
      "2130/2130 [==============================] - 1s 250us/step - loss: 57.4211 - coeff_determination: 0.9062 - val_loss: 86.9607 - val_coeff_determination: 0.7105\n",
      "Epoch 207/700\n",
      "2130/2130 [==============================] - 1s 256us/step - loss: 27.8815 - coeff_determination: 0.9430 - val_loss: 156.2012 - val_coeff_determination: 0.6443\n",
      "Epoch 208/700\n",
      "2130/2130 [==============================] - 1s 287us/step - loss: 49.9117 - coeff_determination: 0.9218 - val_loss: 96.1935 - val_coeff_determination: 0.6507\n",
      "Epoch 209/700\n",
      "2130/2130 [==============================] - 1s 280us/step - loss: 20.1195 - coeff_determination: 0.9426 - val_loss: 84.5431 - val_coeff_determination: 0.6969\n",
      "Epoch 210/700\n",
      "2130/2130 [==============================] - 1s 289us/step - loss: 24.2001 - coeff_determination: 0.9507 - val_loss: 159.3851 - val_coeff_determination: 0.6185\n",
      "Epoch 211/700\n",
      "2130/2130 [==============================] - 1s 266us/step - loss: 24.3333 - coeff_determination: 0.9429 - val_loss: 125.0846 - val_coeff_determination: 0.6362\n",
      "Epoch 212/700\n",
      "2130/2130 [==============================] - 1s 286us/step - loss: 17.2383 - coeff_determination: 0.9508 - val_loss: 124.0123 - val_coeff_determination: 0.6680\n",
      "Epoch 213/700\n",
      "2130/2130 [==============================] - 1s 258us/step - loss: 28.9591 - coeff_determination: 0.9349 - val_loss: 174.5511 - val_coeff_determination: 0.6284\n",
      "Epoch 214/700\n",
      "2130/2130 [==============================] - 1s 292us/step - loss: 21.2733 - coeff_determination: 0.9534 - val_loss: 75.3785 - val_coeff_determination: 0.7110\n",
      "Epoch 215/700\n",
      "2130/2130 [==============================] - 1s 306us/step - loss: 70.1711 - coeff_determination: 0.8895 - val_loss: 325.6233 - val_coeff_determination: 0.4834\n",
      "Epoch 216/700\n",
      "2130/2130 [==============================] - 1s 307us/step - loss: 40.4072 - coeff_determination: 0.9310 - val_loss: 136.6678 - val_coeff_determination: 0.6932\n",
      "Epoch 217/700\n",
      "2130/2130 [==============================] - 1s 298us/step - loss: 19.2500 - coeff_determination: 0.9493 - val_loss: 102.6243 - val_coeff_determination: 0.7181\n",
      "Epoch 218/700\n",
      "2130/2130 [==============================] - 1s 298us/step - loss: 17.6367 - coeff_determination: 0.9521 - val_loss: 118.2203 - val_coeff_determination: 0.6849\n",
      "Epoch 219/700\n",
      "2130/2130 [==============================] - 1s 271us/step - loss: 23.5176 - coeff_determination: 0.9438 - val_loss: 111.4913 - val_coeff_determination: 0.6840\n",
      "Epoch 220/700\n",
      "2130/2130 [==============================] - 1s 269us/step - loss: 31.5590 - coeff_determination: 0.9432 - val_loss: 91.0036 - val_coeff_determination: 0.7295\n",
      "Epoch 221/700\n",
      "2130/2130 [==============================] - 1s 346us/step - loss: 38.9559 - coeff_determination: 0.9328 - val_loss: 156.7373 - val_coeff_determination: 0.6385\n",
      "Epoch 222/700\n",
      "2130/2130 [==============================] - 1s 357us/step - loss: 40.4403 - coeff_determination: 0.9274 - val_loss: 101.7732 - val_coeff_determination: 0.7152\n",
      "Epoch 223/700\n",
      "2130/2130 [==============================] - 1s 291us/step - loss: 39.9229 - coeff_determination: 0.9364 - val_loss: 151.4908 - val_coeff_determination: 0.6447\n",
      "Epoch 224/700\n",
      "2130/2130 [==============================] - 1s 367us/step - loss: 25.0939 - coeff_determination: 0.9450 - val_loss: 87.3351 - val_coeff_determination: 0.7306\n",
      "Epoch 225/700\n",
      "2130/2130 [==============================] - 1s 267us/step - loss: 18.9585 - coeff_determination: 0.9520 - val_loss: 129.9492 - val_coeff_determination: 0.6630\n",
      "Epoch 226/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130/2130 [==============================] - 1s 269us/step - loss: 18.0187 - coeff_determination: 0.9523 - val_loss: 114.3936 - val_coeff_determination: 0.6589\n",
      "Epoch 227/700\n",
      "2130/2130 [==============================] - 1s 263us/step - loss: 29.5096 - coeff_determination: 0.9439 - val_loss: 139.9513 - val_coeff_determination: 0.6441\n",
      "Epoch 228/700\n",
      "2130/2130 [==============================] - 1s 264us/step - loss: 55.6280 - coeff_determination: 0.9086 - val_loss: 95.1321 - val_coeff_determination: 0.6897\n",
      "Epoch 229/700\n",
      "2130/2130 [==============================] - 1s 268us/step - loss: 19.7482 - coeff_determination: 0.9544 - val_loss: 74.2408 - val_coeff_determination: 0.7202\n",
      "Epoch 230/700\n",
      "2130/2130 [==============================] - 1s 337us/step - loss: 30.0473 - coeff_determination: 0.9427 - val_loss: 78.8738 - val_coeff_determination: 0.7220\n",
      "Epoch 231/700\n",
      "2130/2130 [==============================] - 1s 332us/step - loss: 20.5912 - coeff_determination: 0.9498 - val_loss: 111.5515 - val_coeff_determination: 0.6761\n",
      "Epoch 232/700\n",
      "2130/2130 [==============================] - 1s 330us/step - loss: 39.6796 - coeff_determination: 0.9253 - val_loss: 107.8591 - val_coeff_determination: 0.6961\n",
      "Epoch 233/700\n",
      "2130/2130 [==============================] - 1s 352us/step - loss: 58.7732 - coeff_determination: 0.9194 - val_loss: 212.4480 - val_coeff_determination: 0.6120\n",
      "Epoch 234/700\n",
      "2130/2130 [==============================] - 1s 349us/step - loss: 23.7413 - coeff_determination: 0.9429 - val_loss: 99.2540 - val_coeff_determination: 0.7114\n",
      "Epoch 235/700\n",
      "2130/2130 [==============================] - 1s 316us/step - loss: 31.0165 - coeff_determination: 0.9362 - val_loss: 119.5906 - val_coeff_determination: 0.6628\n",
      "Epoch 236/700\n",
      "2130/2130 [==============================] - 1s 421us/step - loss: 49.2545 - coeff_determination: 0.9167 - val_loss: 66.7690 - val_coeff_determination: 0.7447\n",
      "Epoch 237/700\n",
      "2130/2130 [==============================] - 1s 391us/step - loss: 52.1390 - coeff_determination: 0.9171 - val_loss: 97.6254 - val_coeff_determination: 0.6999\n",
      "Epoch 238/700\n",
      "2130/2130 [==============================] - 1s 412us/step - loss: 18.1916 - coeff_determination: 0.9541 - val_loss: 100.2989 - val_coeff_determination: 0.6976\n",
      "Epoch 239/700\n",
      "2130/2130 [==============================] - 1s 300us/step - loss: 34.9175 - coeff_determination: 0.9417 - val_loss: 253.0734 - val_coeff_determination: 0.5341\n",
      "Epoch 240/700\n",
      "2130/2130 [==============================] - 1s 318us/step - loss: 90.1765 - coeff_determination: 0.8556 - val_loss: 80.4840 - val_coeff_determination: 0.6917\n",
      "Epoch 241/700\n",
      "2130/2130 [==============================] - 1s 294us/step - loss: 27.4699 - coeff_determination: 0.9264 - val_loss: 94.1193 - val_coeff_determination: 0.7000\n",
      "Epoch 242/700\n",
      "2130/2130 [==============================] - 1s 296us/step - loss: 38.8124 - coeff_determination: 0.9288 - val_loss: 111.2921 - val_coeff_determination: 0.6533\n",
      "Epoch 243/700\n",
      "2130/2130 [==============================] - 1s 374us/step - loss: 37.2125 - coeff_determination: 0.9356 - val_loss: 98.2241 - val_coeff_determination: 0.7000\n",
      "Epoch 244/700\n",
      "2130/2130 [==============================] - 1s 449us/step - loss: 23.3615 - coeff_determination: 0.9448 - val_loss: 109.9285 - val_coeff_determination: 0.6273\n",
      "Epoch 245/700\n",
      "2130/2130 [==============================] - 1s 409us/step - loss: 25.3861 - coeff_determination: 0.9403 - val_loss: 172.4653 - val_coeff_determination: 0.6365\n",
      "Epoch 246/700\n",
      "2130/2130 [==============================] - 1s 409us/step - loss: 30.3109 - coeff_determination: 0.9437 - val_loss: 197.2765 - val_coeff_determination: 0.5836\n",
      "Epoch 247/700\n",
      "2130/2130 [==============================] - 1s 473us/step - loss: 26.7333 - coeff_determination: 0.9419 - val_loss: 154.8068 - val_coeff_determination: 0.6356\n",
      "Epoch 248/700\n",
      "2130/2130 [==============================] - 1s 316us/step - loss: 54.1571 - coeff_determination: 0.9199 - val_loss: 327.1122 - val_coeff_determination: 0.4556\n",
      "Epoch 249/700\n",
      "2130/2130 [==============================] - 1s 316us/step - loss: 56.2775 - coeff_determination: 0.9084 - val_loss: 242.9786 - val_coeff_determination: 0.5341\n",
      "Epoch 250/700\n",
      "2130/2130 [==============================] - 1s 314us/step - loss: 60.9755 - coeff_determination: 0.9113 - val_loss: 139.9862 - val_coeff_determination: 0.6327\n",
      "Epoch 251/700\n",
      "2130/2130 [==============================] - 1s 352us/step - loss: 30.4605 - coeff_determination: 0.9440 - val_loss: 98.4434 - val_coeff_determination: 0.6762\n",
      "Epoch 252/700\n",
      "2130/2130 [==============================] - 1s 376us/step - loss: 18.9760 - coeff_determination: 0.9448 - val_loss: 125.6975 - val_coeff_determination: 0.6165\n",
      "Epoch 253/700\n",
      "2130/2130 [==============================] - 1s 390us/step - loss: 44.3268 - coeff_determination: 0.9152 - val_loss: 85.1476 - val_coeff_determination: 0.7097\n",
      "Epoch 254/700\n",
      "2130/2130 [==============================] - 1s 339us/step - loss: 32.4487 - coeff_determination: 0.9486 - val_loss: 93.0835 - val_coeff_determination: 0.7028\n",
      "Epoch 255/700\n",
      "2130/2130 [==============================] - 1s 358us/step - loss: 32.6098 - coeff_determination: 0.9402 - val_loss: 91.4288 - val_coeff_determination: 0.6823\n",
      "Epoch 256/700\n",
      "2130/2130 [==============================] - 1s 379us/step - loss: 32.0786 - coeff_determination: 0.9402 - val_loss: 188.1052 - val_coeff_determination: 0.5786\n",
      "Epoch 257/700\n",
      "2130/2130 [==============================] - 1s 499us/step - loss: 43.5823 - coeff_determination: 0.9193 - val_loss: 216.7964 - val_coeff_determination: 0.5665\n",
      "Epoch 258/700\n",
      "2130/2130 [==============================] - 1s 374us/step - loss: 20.6609 - coeff_determination: 0.9464 - val_loss: 84.0867 - val_coeff_determination: 0.6825\n",
      "Epoch 259/700\n",
      "2130/2130 [==============================] - 1s 422us/step - loss: 28.3194 - coeff_determination: 0.9414 - val_loss: 107.6228 - val_coeff_determination: 0.6277\n",
      "Epoch 260/700\n",
      "2130/2130 [==============================] - 1s 362us/step - loss: 38.1827 - coeff_determination: 0.9328 - val_loss: 111.6378 - val_coeff_determination: 0.6871\n",
      "Epoch 261/700\n",
      "2130/2130 [==============================] - 1s 448us/step - loss: 23.4211 - coeff_determination: 0.9514 - val_loss: 117.9959 - val_coeff_determination: 0.6618\n",
      "Epoch 262/700\n",
      "2130/2130 [==============================] - 1s 417us/step - loss: 21.0834 - coeff_determination: 0.9488 - val_loss: 70.3927 - val_coeff_determination: 0.7331\n",
      "Epoch 263/700\n",
      "2130/2130 [==============================] - 1s 396us/step - loss: 16.0162 - coeff_determination: 0.9483 - val_loss: 82.5188 - val_coeff_determination: 0.6851\n",
      "Epoch 264/700\n",
      "2130/2130 [==============================] - 1s 411us/step - loss: 37.5426 - coeff_determination: 0.9335 - val_loss: 110.9178 - val_coeff_determination: 0.6710\n",
      "Epoch 265/700\n",
      "2130/2130 [==============================] - 1s 513us/step - loss: 45.4167 - coeff_determination: 0.9275 - val_loss: 114.4692 - val_coeff_determination: 0.6200\n",
      "Epoch 266/700\n",
      "2130/2130 [==============================] - 1s 324us/step - loss: 26.8862 - coeff_determination: 0.9433 - val_loss: 119.7325 - val_coeff_determination: 0.6677\n",
      "Epoch 267/700\n",
      "2130/2130 [==============================] - 1s 325us/step - loss: 33.6859 - coeff_determination: 0.9379 - val_loss: 82.9129 - val_coeff_determination: 0.7180\n",
      "Epoch 268/700\n",
      "2130/2130 [==============================] - 1s 349us/step - loss: 19.7074 - coeff_determination: 0.9491 - val_loss: 113.1285 - val_coeff_determination: 0.6370\n",
      "Epoch 269/700\n",
      "2130/2130 [==============================] - 1s 316us/step - loss: 18.5150 - coeff_determination: 0.9505 - val_loss: 87.7293 - val_coeff_determination: 0.6755\n",
      "Epoch 270/700\n",
      "2130/2130 [==============================] - 1s 305us/step - loss: 32.3268 - coeff_determination: 0.9334 - val_loss: 350.8396 - val_coeff_determination: 0.4444\n",
      "Epoch 271/700\n",
      "2130/2130 [==============================] - 1s 301us/step - loss: 57.7265 - coeff_determination: 0.8923 - val_loss: 135.7172 - val_coeff_determination: 0.6316\n",
      "Epoch 272/700\n",
      "2130/2130 [==============================] - 1s 293us/step - loss: 37.1697 - coeff_determination: 0.9334 - val_loss: 90.8628 - val_coeff_determination: 0.6897\n",
      "Epoch 273/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130/2130 [==============================] - 1s 276us/step - loss: 23.8790 - coeff_determination: 0.9427 - val_loss: 240.3918 - val_coeff_determination: 0.5875\n",
      "Epoch 274/700\n",
      "2130/2130 [==============================] - 1s 261us/step - loss: 58.5796 - coeff_determination: 0.9227 - val_loss: 93.2722 - val_coeff_determination: 0.7209\n",
      "Epoch 275/700\n",
      "2130/2130 [==============================] - 1s 273us/step - loss: 43.2576 - coeff_determination: 0.9385 - val_loss: 86.1402 - val_coeff_determination: 0.6734\n",
      "Epoch 276/700\n",
      "2130/2130 [==============================] - 1s 272us/step - loss: 17.7267 - coeff_determination: 0.9511 - val_loss: 155.5970 - val_coeff_determination: 0.6630\n",
      "Epoch 277/700\n",
      "2130/2130 [==============================] - 1s 310us/step - loss: 32.6710 - coeff_determination: 0.9289 - val_loss: 85.6208 - val_coeff_determination: 0.7138\n",
      "Epoch 278/700\n",
      "2130/2130 [==============================] - 1s 282us/step - loss: 29.4550 - coeff_determination: 0.9433 - val_loss: 87.9274 - val_coeff_determination: 0.6787\n",
      "Epoch 279/700\n",
      "2130/2130 [==============================] - 1s 277us/step - loss: 29.4694 - coeff_determination: 0.9403 - val_loss: 145.4423 - val_coeff_determination: 0.6522\n",
      "Epoch 280/700\n",
      "2130/2130 [==============================] - 1s 292us/step - loss: 35.1998 - coeff_determination: 0.9440 - val_loss: 187.3705 - val_coeff_determination: 0.5699\n",
      "Epoch 281/700\n",
      "2130/2130 [==============================] - 1s 279us/step - loss: 19.7984 - coeff_determination: 0.9500 - val_loss: 228.3836 - val_coeff_determination: 0.5329\n",
      "Epoch 282/700\n",
      "2130/2130 [==============================] - 1s 269us/step - loss: 20.5706 - coeff_determination: 0.9520 - val_loss: 88.3864 - val_coeff_determination: 0.6899\n",
      "Epoch 283/700\n",
      "2130/2130 [==============================] - 1s 267us/step - loss: 43.5705 - coeff_determination: 0.9324 - val_loss: 160.6170 - val_coeff_determination: 0.6143\n",
      "Epoch 284/700\n",
      "2130/2130 [==============================] - 1s 286us/step - loss: 19.6705 - coeff_determination: 0.9537 - val_loss: 169.3446 - val_coeff_determination: 0.6045\n",
      "Epoch 285/700\n",
      "2130/2130 [==============================] - 1s 298us/step - loss: 59.8184 - coeff_determination: 0.9037 - val_loss: 171.4704 - val_coeff_determination: 0.5959\n",
      "Epoch 286/700\n",
      "2130/2130 [==============================] - 1s 279us/step - loss: 17.0306 - coeff_determination: 0.9513 - val_loss: 132.3814 - val_coeff_determination: 0.6145\n",
      "Epoch 287/700\n",
      "2130/2130 [==============================] - 1s 276us/step - loss: 28.0887 - coeff_determination: 0.9371 - val_loss: 121.7843 - val_coeff_determination: 0.6563\n",
      "Epoch 288/700\n",
      "2130/2130 [==============================] - 1s 298us/step - loss: 54.6690 - coeff_determination: 0.9140 - val_loss: 168.0558 - val_coeff_determination: 0.6506\n",
      "Epoch 289/700\n",
      "2130/2130 [==============================] - 1s 296us/step - loss: 81.1429 - coeff_determination: 0.8959 - val_loss: 106.7452 - val_coeff_determination: 0.6501\n",
      "Epoch 290/700\n",
      "2130/2130 [==============================] - 1s 335us/step - loss: 115.3413 - coeff_determination: 0.8333 - val_loss: 289.2568 - val_coeff_determination: 0.5119\n",
      "Epoch 291/700\n",
      "2130/2130 [==============================] - 1s 321us/step - loss: 26.1156 - coeff_determination: 0.9431 - val_loss: 135.5345 - val_coeff_determination: 0.6205\n",
      "Epoch 292/700\n",
      "2130/2130 [==============================] - 1s 284us/step - loss: 43.0242 - coeff_determination: 0.9216 - val_loss: 96.9229 - val_coeff_determination: 0.6919\n",
      "Epoch 293/700\n",
      "2130/2130 [==============================] - 1s 293us/step - loss: 40.6830 - coeff_determination: 0.9268 - val_loss: 366.1157 - val_coeff_determination: 0.4097\n",
      "Epoch 294/700\n",
      "2130/2130 [==============================] - 1s 308us/step - loss: 45.3511 - coeff_determination: 0.9149 - val_loss: 115.4162 - val_coeff_determination: 0.6576\n",
      "Epoch 295/700\n",
      "2130/2130 [==============================] - 1s 289us/step - loss: 20.2248 - coeff_determination: 0.9494 - val_loss: 82.4552 - val_coeff_determination: 0.6979\n",
      "Epoch 296/700\n",
      "2130/2130 [==============================] - 1s 286us/step - loss: 32.1222 - coeff_determination: 0.9420 - val_loss: 130.7937 - val_coeff_determination: 0.6258\n",
      "Epoch 297/700\n",
      "2130/2130 [==============================] - 1s 280us/step - loss: 60.8882 - coeff_determination: 0.9056 - val_loss: 149.2751 - val_coeff_determination: 0.6231\n",
      "Epoch 298/700\n",
      "2130/2130 [==============================] - 1s 268us/step - loss: 69.2576 - coeff_determination: 0.9159 - val_loss: 148.8504 - val_coeff_determination: 0.6178\n",
      "Epoch 299/700\n",
      "2130/2130 [==============================] - 1s 261us/step - loss: 105.3625 - coeff_determination: 0.8783 - val_loss: 117.3388 - val_coeff_determination: 0.6194\n",
      "Epoch 300/700\n",
      "2130/2130 [==============================] - 1s 358us/step - loss: 35.6604 - coeff_determination: 0.9362 - val_loss: 100.6937 - val_coeff_determination: 0.6639\n",
      "Epoch 301/700\n",
      "2130/2130 [==============================] - 1s 332us/step - loss: 95.6126 - coeff_determination: 0.8901 - val_loss: 105.6788 - val_coeff_determination: 0.6645\n",
      "Epoch 302/700\n",
      "2130/2130 [==============================] - 1s 314us/step - loss: 30.4915 - coeff_determination: 0.9473 - val_loss: 124.2911 - val_coeff_determination: 0.6539\n",
      "Epoch 303/700\n",
      "2130/2130 [==============================] - 1s 293us/step - loss: 20.0502 - coeff_determination: 0.9469 - val_loss: 104.7513 - val_coeff_determination: 0.6845\n",
      "Epoch 304/700\n",
      "2130/2130 [==============================] - 1s 299us/step - loss: 26.5426 - coeff_determination: 0.9482 - val_loss: 97.2290 - val_coeff_determination: 0.6807\n",
      "Epoch 305/700\n",
      "2130/2130 [==============================] - 1s 266us/step - loss: 35.8058 - coeff_determination: 0.9394 - val_loss: 119.2579 - val_coeff_determination: 0.6604\n",
      "Epoch 306/700\n",
      "2130/2130 [==============================] - 1s 274us/step - loss: 24.5099 - coeff_determination: 0.9538 - val_loss: 116.0105 - val_coeff_determination: 0.6556\n",
      "Epoch 307/700\n",
      "2130/2130 [==============================] - 1s 277us/step - loss: 43.1467 - coeff_determination: 0.9392 - val_loss: 127.7601 - val_coeff_determination: 0.6366\n",
      "Epoch 308/700\n",
      "2130/2130 [==============================] - 1s 268us/step - loss: 19.4040 - coeff_determination: 0.9536 - val_loss: 108.4944 - val_coeff_determination: 0.6599\n",
      "Epoch 309/700\n",
      "2130/2130 [==============================] - 1s 271us/step - loss: 31.9133 - coeff_determination: 0.9478 - val_loss: 105.6716 - val_coeff_determination: 0.6720\n",
      "Epoch 310/700\n",
      "2130/2130 [==============================] - 1s 273us/step - loss: 36.8786 - coeff_determination: 0.9294 - val_loss: 95.2102 - val_coeff_determination: 0.6582\n",
      "Epoch 311/700\n",
      "2130/2130 [==============================] - 1s 272us/step - loss: 206.8632 - coeff_determination: 0.7614 - val_loss: 311.0446 - val_coeff_determination: 0.4769\n",
      "Epoch 312/700\n",
      "2130/2130 [==============================] - 1s 312us/step - loss: 87.1342 - coeff_determination: 0.8894 - val_loss: 246.2379 - val_coeff_determination: 0.4999\n",
      "Epoch 313/700\n",
      "2130/2130 [==============================] - 1s 290us/step - loss: 100.8222 - coeff_determination: 0.8803 - val_loss: 137.4167 - val_coeff_determination: 0.6379\n",
      "Epoch 314/700\n",
      "2130/2130 [==============================] - 1s 353us/step - loss: 144.0059 - coeff_determination: 0.8384 - val_loss: 86.7559 - val_coeff_determination: 0.6735\n",
      "Epoch 315/700\n",
      "2130/2130 [==============================] - 1s 335us/step - loss: 36.3211 - coeff_determination: 0.9349 - val_loss: 267.3039 - val_coeff_determination: 0.5164\n",
      "Epoch 316/700\n",
      "2130/2130 [==============================] - 1s 489us/step - loss: 31.3348 - coeff_determination: 0.9433 - val_loss: 169.0459 - val_coeff_determination: 0.5851\n",
      "Epoch 317/700\n",
      "2130/2130 [==============================] - 1s 400us/step - loss: 37.0631 - coeff_determination: 0.9322 - val_loss: 277.2409 - val_coeff_determination: 0.4936\n",
      "Epoch 318/700\n",
      "2130/2130 [==============================] - 1s 403us/step - loss: 194.2453 - coeff_determination: 0.8082 - val_loss: 95.9303 - val_coeff_determination: 0.6526\n",
      "Epoch 319/700\n",
      "2130/2130 [==============================] - 1s 423us/step - loss: 29.4536 - coeff_determination: 0.9387 - val_loss: 93.2989 - val_coeff_determination: 0.6524\n",
      "Epoch 320/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130/2130 [==============================] - 1s 391us/step - loss: 33.4883 - coeff_determination: 0.9416 - val_loss: 115.3756 - val_coeff_determination: 0.6358\n",
      "Epoch 321/700\n",
      "2130/2130 [==============================] - 1s 361us/step - loss: 41.3546 - coeff_determination: 0.9356 - val_loss: 160.7968 - val_coeff_determination: 0.6149\n",
      "Epoch 322/700\n",
      "2130/2130 [==============================] - 1s 384us/step - loss: 33.6427 - coeff_determination: 0.9411 - val_loss: 129.6640 - val_coeff_determination: 0.6114\n",
      "Epoch 323/700\n",
      "2130/2130 [==============================] - 1s 367us/step - loss: 32.4370 - coeff_determination: 0.9501 - val_loss: 95.1665 - val_coeff_determination: 0.6531\n",
      "Epoch 324/700\n",
      "2130/2130 [==============================] - 1s 351us/step - loss: 12.2608 - coeff_determination: 0.9604 - val_loss: 98.3319 - val_coeff_determination: 0.6483\n",
      "Epoch 325/700\n",
      "2130/2130 [==============================] - 1s 318us/step - loss: 24.3784 - coeff_determination: 0.9509 - val_loss: 122.8185 - val_coeff_determination: 0.6236\n",
      "Epoch 326/700\n",
      "2130/2130 [==============================] - 1s 362us/step - loss: 23.3659 - coeff_determination: 0.9553 - val_loss: 89.4062 - val_coeff_determination: 0.6570\n",
      "Epoch 327/700\n",
      "2130/2130 [==============================] - 1s 402us/step - loss: 34.2585 - coeff_determination: 0.9407 - val_loss: 186.6835 - val_coeff_determination: 0.5944\n",
      "Epoch 328/700\n",
      "2130/2130 [==============================] - 1s 459us/step - loss: 33.8160 - coeff_determination: 0.9388 - val_loss: 172.3643 - val_coeff_determination: 0.5756\n",
      "Epoch 329/700\n",
      "2130/2130 [==============================] - 1s 470us/step - loss: 44.0481 - coeff_determination: 0.9304 - val_loss: 126.7839 - val_coeff_determination: 0.6517\n",
      "Epoch 330/700\n",
      "2130/2130 [==============================] - 1s 369us/step - loss: 19.9668 - coeff_determination: 0.9544 - val_loss: 93.9308 - val_coeff_determination: 0.6742\n",
      "Epoch 331/700\n",
      "2130/2130 [==============================] - 1s 416us/step - loss: 36.6072 - coeff_determination: 0.9386 - val_loss: 85.5645 - val_coeff_determination: 0.6899\n",
      "Epoch 332/700\n",
      "2130/2130 [==============================] - 1s 372us/step - loss: 53.8102 - coeff_determination: 0.9229 - val_loss: 162.8086 - val_coeff_determination: 0.6006\n",
      "Epoch 333/700\n",
      "2130/2130 [==============================] - 1s 375us/step - loss: 25.6244 - coeff_determination: 0.9445 - val_loss: 91.1091 - val_coeff_determination: 0.6615\n",
      "Epoch 334/700\n",
      "2130/2130 [==============================] - 1s 392us/step - loss: 27.2726 - coeff_determination: 0.9505 - val_loss: 190.2495 - val_coeff_determination: 0.5937\n",
      "Epoch 335/700\n",
      "2130/2130 [==============================] - 1s 431us/step - loss: 21.2599 - coeff_determination: 0.9557 - val_loss: 169.7288 - val_coeff_determination: 0.6061\n",
      "Epoch 336/700\n",
      "2130/2130 [==============================] - 1s 406us/step - loss: 29.5930 - coeff_determination: 0.9457 - val_loss: 149.9784 - val_coeff_determination: 0.5916\n",
      "Epoch 337/700\n",
      "2130/2130 [==============================] - 1s 401us/step - loss: 16.0688 - coeff_determination: 0.9594 - val_loss: 89.0558 - val_coeff_determination: 0.6947\n",
      "Epoch 338/700\n",
      "2130/2130 [==============================] - 1s 396us/step - loss: 19.0817 - coeff_determination: 0.9612 - val_loss: 89.3136 - val_coeff_determination: 0.6918\n",
      "Epoch 339/700\n",
      "2130/2130 [==============================] - 1s 405us/step - loss: 19.4440 - coeff_determination: 0.9519 - val_loss: 98.0741 - val_coeff_determination: 0.6577\n",
      "Epoch 340/700\n",
      "2130/2130 [==============================] - 1s 389us/step - loss: 34.8435 - coeff_determination: 0.9375 - val_loss: 196.1260 - val_coeff_determination: 0.5819\n",
      "Epoch 341/700\n",
      "2130/2130 [==============================] - 1s 376us/step - loss: 90.4865 - coeff_determination: 0.8850 - val_loss: 172.3951 - val_coeff_determination: 0.5496\n",
      "Epoch 342/700\n",
      "2130/2130 [==============================] - 1s 323us/step - loss: 183.7096 - coeff_determination: 0.8101 - val_loss: 193.2651 - val_coeff_determination: 0.5814\n",
      "Epoch 343/700\n",
      "2130/2130 [==============================] - 1s 366us/step - loss: 112.0588 - coeff_determination: 0.8399 - val_loss: 138.8868 - val_coeff_determination: 0.6171\n",
      "Epoch 344/700\n",
      "2130/2130 [==============================] - 1s 405us/step - loss: 45.2345 - coeff_determination: 0.9199 - val_loss: 138.0511 - val_coeff_determination: 0.6420\n",
      "Epoch 345/700\n",
      "2130/2130 [==============================] - 1s 419us/step - loss: 81.7644 - coeff_determination: 0.8945 - val_loss: 420.6500 - val_coeff_determination: 0.3654\n",
      "Epoch 346/700\n",
      "2130/2130 [==============================] - 1s 375us/step - loss: 139.6808 - coeff_determination: 0.8339 - val_loss: 137.9516 - val_coeff_determination: 0.5946\n",
      "Epoch 347/700\n",
      "2130/2130 [==============================] - 1s 379us/step - loss: 154.9083 - coeff_determination: 0.8365 - val_loss: 316.2338 - val_coeff_determination: 0.4949\n",
      "Epoch 348/700\n",
      "2130/2130 [==============================] - 1s 373us/step - loss: 34.7646 - coeff_determination: 0.9255 - val_loss: 133.3454 - val_coeff_determination: 0.6513\n",
      "Epoch 349/700\n",
      "2130/2130 [==============================] - 1s 361us/step - loss: 58.5561 - coeff_determination: 0.8958 - val_loss: 103.1755 - val_coeff_determination: 0.6419\n",
      "Epoch 350/700\n",
      "2130/2130 [==============================] - 1s 366us/step - loss: 50.0682 - coeff_determination: 0.9164 - val_loss: 100.2177 - val_coeff_determination: 0.6506\n",
      "Epoch 351/700\n",
      "2130/2130 [==============================] - 1s 366us/step - loss: 55.6809 - coeff_determination: 0.9142 - val_loss: 242.8346 - val_coeff_determination: 0.4987\n",
      "Epoch 352/700\n",
      "2130/2130 [==============================] - 1s 381us/step - loss: 38.0882 - coeff_determination: 0.9227 - val_loss: 216.6488 - val_coeff_determination: 0.5070\n",
      "Epoch 353/700\n",
      "2130/2130 [==============================] - 1s 398us/step - loss: 182.9331 - coeff_determination: 0.7769 - val_loss: 438.7228 - val_coeff_determination: 0.3425\n",
      "Epoch 354/700\n",
      "2130/2130 [==============================] - 1s 348us/step - loss: 79.5934 - coeff_determination: 0.8924 - val_loss: 117.6478 - val_coeff_determination: 0.6303\n",
      "Epoch 355/700\n",
      "2130/2130 [==============================] - 1s 383us/step - loss: 61.3851 - coeff_determination: 0.9098 - val_loss: 166.3847 - val_coeff_determination: 0.5804\n",
      "Epoch 356/700\n",
      "1376/2130 [==================>...........] - ETA: 0s - loss: 54.1708 - coeff_determination: 0.9280"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e7ec6b23effc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoeff_determination\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/keras-jupyter/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/keras-jupyter/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-jupyter/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-jupyter/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras-jupyter/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(df, 600, 'simple')\n",
    "\n",
    "verbose = 1 # выводить ли инфу в процессе обучения (на гитхаб лучше не заливать когда verbose > 0, а просто перетренировать с = 0)\n",
    "            # 0 - silence \n",
    "            # 1 - progress bar\n",
    "            # 2 - line per epoch \n",
    "model = create_dropout_model(X_train.shape[1])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[coeff_determination])\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=700, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 75us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[199.50483896891276, 0.49883066018422445]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_on_test(model, enc='simple'):\n",
    "    df = pd.read_csv('./robot_data/test_data.csv')\n",
    "    df = df.drop(columns=['year', 'target'])\n",
    "    \n",
    "    if enc == 'simple':\n",
    "        df = simple_encode(df.copy())\n",
    "        \n",
    "    elif enc == 'onehot':\n",
    "        df = oneHotEncode(df.copy())\n",
    "        \n",
    "    y_pred = model.predict(df)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submition(model, enc='simple', subm_name=None):\n",
    "    df = pd.read_csv('./robot_data/test_data.csv')\n",
    "    years = df['year']\n",
    "    \n",
    "    y_pred = get_preds_on_test(model)\n",
    "    y_pred = y_pred.reshape(1000)\n",
    "    \n",
    "    d = {'year': years.values, 'target': y_pred}\n",
    "    ans = pd.DataFrame(d)\n",
    "    ans = ans.set_index('year')\n",
    "    \n",
    "    subm_name = subm_name if subm_name else 'submission_.csv'\n",
    "    ans.to_csv(subm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submition(model, subm_name='subm_gbr_bad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
